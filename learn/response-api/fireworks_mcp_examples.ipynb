{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Fireworks OpenAI-Compatible API with MCP Support Examples\n",
    "\n",
    "This notebook demonstrates how to use Fireworks' new OpenAI-compatible API with Model Context Protocol (MCP) support. We'll focus on GitMCP examples using the `reward-kit` repository.\n",
    "\n",
    "## Key Features Demonstrated:\n",
    "- **GitMCP Integration**: Connect to GitHub repositories through MCP\n",
    "- **Open Model Support**: Using Qwen 3 235B model with external tools\n",
    "- **Real-world Use Cases**: Documentation search, code analysis, and more\n",
    "- **Production-Ready Examples**: Scalable patterns for enterprise applications\n",
    "\n",
    "Let's start by setting up our environment and exploring the capabilities!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install all the dependencies\n",
    "!pip install openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Installation\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from pprint import pprint\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.fireworks.ai/inference/v1\",\n",
    "    api_key=os.getenv(\"FIREWORKS_API_KEY\", \"YOUR_FIREWORKS_API_KEY_HERE\")\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üî• Example 1: Basic GitMCP Integration with Reward-Kit\n",
    "\n",
    "Let's start with a simple example connecting to the reward-kit repository through GitMCP. This demonstrates the core functionality of using an open model (Qwen 3 235B) with external tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: What are the key features of reward-kit?\n",
      "============================================================\n",
      "\n",
      "\n",
      "reward-kit is a tool for authoring, testing, and deploying reward functions to evaluate LLM outputs. Its 2 main features are:\n",
      "\n",
      "1. **Easy-to-use Decorator (`@reward_function`)**  \n",
      "   Simplifies reward function creation by annotating Python functions with validation metrics and evaluation logic (e.g., `def exact_tool_match_reward(...)` for tool-call validation).\n",
      "\n",
      "2. **Flexible Multi-Metric Evaluation**  \n",
      "   Supports custom metrics (e.g., word count, specificity markers) and integrates with external libraries like DeepEval/GEval for LLM-as-a-judge scoring. Evaluation results include granular metric breakdowns. \n",
      "\n",
      "The toolkit also enables local testing, dataset integration, and deployment to platforms like Fireworks AI.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Basic Documentation Query\n",
    "# Ask about reward-kit's key features using GitMCP\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"accounts/fireworks/models/qwen3-235b-a22b\",\n",
    "    input=\"What is reward-kit and what are its 2 main features? Keep it short Please analyze the fw-ai-external/reward-kit repository.\",\n",
    "    tools=[{\"type\": \"sse\", \"server_url\": \"https://gitmcp.io/docs\"}]\n",
    ")\n",
    "\n",
    "print(\"üîç Query: What are the key features of reward-kit?\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(response.output[-1].content[0].text.split(\"</think>\")[-1])\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üî• Example 2: Installation and Setup Guide\n",
    "\n",
    "Let's ask the model to help us understand how to get started with reward-kit, pulling information directly from the repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Query: How to install and get started with reward-kit\n",
      "============================================================\n",
      "\n",
      "\n",
      "To install and get started with `fw-ai-external/reward-kit`, follow these steps:\n",
      "\n",
      "---\n",
      "\n",
      "### **Installation**\n",
      "1. **Install the base package**:\n",
      "   ```bash\n",
      "   pip install reward-kit\n",
      "   ```\n",
      "\n",
      "2. **Optional: Install TRL extras** (required for TRL-based training examples):\n",
      "   ```bash\n",
      "   pip install \"reward-kit[trl]\"\n",
      "   ```\n",
      "\n",
      "---\n",
      "\n",
      "### **Prerequisites**\n",
      "- Python 3.x (ensure `pip` is installed)\n",
      "- Optional: For TRL features, install the `[trl]` extra dependency (as shown above).\n",
      "- Optional: Fireworks AI credentials (if deploying evaluators or using Fireworks-hosted models):\n",
      "  ```bash\n",
      "  export FIREWORKS_API_KEY=\"your_api_key\"\n",
      "  export FIREWORKS_ACCOUNT_ID=\"your_account_id\"\n",
      "  ```\n",
      "\n",
      "---\n",
      "\n",
      "### **Basic Usage Example**\n",
      "1. **Define a reward function** using the `@reward_function` decorator:\n",
      "   ```python\n",
      "   from reward_kit import reward_function\n",
      "   from reward_kit.models import EvaluateResult, MetricResult, Message\n",
      "\n",
      "   @reward_function\n",
      "   def word_count_reward(\n",
      "       messages: list[Message],  # Input messages\n",
      "       **kwargs\n",
      "   ) -> EvaluateResult:\n",
      "       \"\"\"Evaluate based on response length.\"\"\"\n",
      "       response = messages[-1].content\n",
      "       word_count = len(response.split())\n",
      "       score = min(word_count / 100.0, 1.0)  # Score capped at 1.0\n",
      "       return EvaluateResult(\n",
      "           score=score,\n",
      "           reason=f\"Word count: {word_count}\",\n",
      "           metrics={\n",
      "               \"word_count\": MetricResult(\n",
      "                   score=score,\n",
      "                   success=word_count > 10,\n",
      "                   reason=f\"Response length: {word_count} words\"\n",
      "               )\n",
      "           }\n",
      "       )\n",
      "   ```\n",
      "\n",
      "2. **Save this code** to a file (e.g., `my_reward.py`).\n",
      "\n",
      "---\n",
      "\n",
      "### **Testing with Sample Data**\n",
      "1. **Create a JSONL file** with sample inputs (e.g., `samples.jsonl`):\n",
      "   ```json\n",
      "   {\"messages\": [{\"role\": \"user\", \"content\": \"Tell me about AI\"}, {\"role\": \"assistant\", \"content\": \"AI refers to systems designed to mimic human intelligence.\"}], \"ground_truth\": {\"word_count\": 10}}\n",
      "   {\"messages\": [{\"role\": \"user\", \"content\": \"What is machine learning?\"}, {\"role\": \"assistant\", \"content\": \"Machine learning is a subset of AI that focuses on building systems that can learn from data.\"}], \"ground_truth\": {\"word_count\": 15}}\n",
      "   ```\n",
      "\n",
      "2. **Preview the evaluation**:\n",
      "   ```bash\n",
      "   reward-kit preview \\\n",
      "     --metrics-folders \"word_count=examples/metrics/word_count\" \\\n",
      "     --samples samples.jsonl\n",
      "   ```\n",
      "\n",
      "---\n",
      "\n",
      "### **Deploying a Reward Function**\n",
      "1. **Deploy via CLI**:\n",
      "   ```bash\n",
      "   reward-kit deploy \\\n",
      "     --id my-evaluator \\\n",
      "     --metrics-folders \"word_count=examples/metrics/word_count\" \\\n",
      "     --force\n",
      "   ```\n",
      "\n",
      "2. **Deploy programmatically**:\n",
      "   ```python\n",
      "   from reward_kit.evaluation import create_evaluation\n",
      "\n",
      "   evaluator = create_evaluation(\n",
      "       evaluator_id=\"my-evaluator\",\n",
      "       metric_folders=[\"word_count=examples/metrics/word_count\"],\n",
      "       display_name=\"Word Count Evaluator\",\n",
      "       description=\"Evaluates responses based on word count\",\n",
      "       force=True\n",
      "   )\n",
      "   ```\n",
      "\n",
      "---\n",
      "\n",
      "### **Advanced: Local Development Server**\n",
      "For testing webhook integrations:\n",
      "```bash\n",
      "reward-kit deploy \\\n",
      "  --id test-local-serve \\\n",
      "  --target local-serve \\\n",
      "  --function-ref my_reward.word_count_reward \\\n",
      "  --verbose \\\n",
      "  --force\n",
      "```\n",
      "This starts a local server with an external tunnel.\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Resources**\n",
      "- **Documentation**: [Full documentation](docs/documentation_home.mdx) for advanced configuration.\n",
      "- **Examples**: Explore the `examples/` directory in the repo for math and coding evaluations.\n",
      "- **CLI Reference**: Run `reward-kit --help` for command details.\n",
      "\n",
      "Let me know if you need further clarification on any step!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Installation and Getting Started\n",
    "response = client.responses.create(\n",
    "    model=\"accounts/fireworks/models/qwen3-235b-a22b\",\n",
    "    input=\"How do I install and get started with fw-ai-external/reward-kit? Please provide step-by-step instructions including any prerequisites and basic usage examples.\",\n",
    "    tools=[{\"type\": \"sse\", \"server_url\": \"https://gitmcp.io/docs\"}]\n",
    ")\n",
    "\n",
    "print(\"üöÄ Query: How to install and get started with reward-kit\")\n",
    "print(\"=\" * 60)\n",
    "print(response.output[-1].content[0].text.split(\"</think>\")[-1])\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üî• Example 3: Code Analysis and Understanding\n",
    "\n",
    "Now let's dive deeper into understanding specific parts of the codebase. We'll ask about reward functions and how they work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Query: How do reward functions work in reward-kit?\n",
      "============================================================\n",
      "\n",
      "\n",
      "Here's a breakdown of how reward functions and the `@reward_function` decorator work in **reward-kit**, along with the implementation details of `exact_tool_match_reward`:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. The `@reward_function` Decorator**\n",
      "The decorator wraps reward function definitions and ensures they:\n",
      "- Accept parameters like `messages`, `ground_truth`, `**kwargs`\n",
      "- Return an `EvaluateResult` object with a numeric score and metrics\n",
      "- Integrate seamlessly into the `reward-kit` CLI and evaluation workflows\n",
      "\n",
      "```python\n",
      "@reward_function\n",
      "def exact_tool_match_reward(\n",
      "    messages: Union[List[Message], List[Dict[str, Any]]],\n",
      "    ground_truth: Optional[Dict[str, Any]] = None,\n",
      "    **kwargs,\n",
      ") -> EvaluateResult:\n",
      "    # Function implementation...\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **2. `exact_tool_match_reward` Implementation**\n",
      "This function evaluates whether tool calls in `messages` match an expected tool call structure in `ground_truth`.\n",
      "\n",
      "#### **Key Steps**:\n",
      "1. **Input Handling**:\n",
      "   ```python\n",
      "   if not messages:\n",
      "       return EvaluateResult(\n",
      "           score=0.0, \n",
      "           reason=\"No messages provided for evaluation.\", \n",
      "           metrics={}\n",
      "       )\n",
      "   ```\n",
      "   - Validates input messages exist\n",
      "\n",
      "2. **Tool Call Extraction**:\n",
      "   ```python\n",
      "   generation_dict = {\n",
      "       \"role\": generation_message_obj.role,\n",
      "       \"content\": generation_message_obj.content,\n",
      "       \"tool_calls\": [...] if generation_message_obj.tool_calls else []\n",
      "   }\n",
      "   ```\n",
      "   - Extracts tool calls from the last assistant message\n",
      "\n",
      "3. **Ground Truth Handling**:\n",
      "   ```python\n",
      "   if ground_truth is None:\n",
      "       # Score 1 if no tool calls exist, 0 otherwise\n",
      "       return EvaluateResult(...)\n",
      "   ```\n",
      "   - Scores based on presence/absence of tool calls when no ground truth exists\n",
      "\n",
      "4. **Comparison**:\n",
      "   ```python\n",
      "   score = float(eval_tool_call(generation_dict, ground_truth))\n",
      "   reason = f\"Exact tool match evaluation score: {score}\"\n",
      "   return EvaluateResult(score=score, reason=reason, metrics={})\n",
      "   ```\n",
      "   - Delegates to `eval_tool_call` for precise comparison logic\n",
      "\n",
      "---\n",
      "\n",
      "### **3. `eval_tool_call` Function**\n",
      "```python\n",
      "def eval_tool_call(generation: dict, ground_truth: dict) -> bool:\n",
      "    expected_gt_tool_calls = ground_truth.get(\"tool_calls\", [])\n",
      "    deserialized_gt_tool_calls = maybe_deserialize_tool_call_arguments(expected_gt_tool_calls)\n",
      "    ground_truth_simple_format = [tc[\"function\"] for tc in deserialized_gt_tool_calls]\n",
      "\n",
      "    raw_generated_tool_calls = generation.get(\"tool_calls\")\n",
      "    if raw_generated_tool_calls:\n",
      "        deserialized_gen_tool_calls = maybe_deserialize_tool_call_arguments(raw_generated_tool_calls)\n",
      "        generated_simple_format = [tc[\"function\"] for tc in deserialized_gen_tool_calls]\n",
      "    elif \"<tool_call>\" in generation.get(\"content\", \"\"):\n",
      "        parsed_tool_calls_from_content = parse_tool_calls(generation[\"content\"])\n",
      "        generated_simple_format = [...]\n",
      "    else:\n",
      "        generated_simple_format = []\n",
      "\n",
      "    return compare_tool_calls(generated_simple_format, ground_truth_simple_format)\n",
      "```\n",
      "\n",
      "### **4. Example Test Case**\n",
      "From documentation:\n",
      "```python\n",
      "# Correct call\n",
      "test_messages_correct = [\n",
      "    Message(role=\"user\", content=\"What's the weather in SF?\"),\n",
      "    Message(\n",
      "        role=\"assistant\", \n",
      "        tool_calls=[{\"name\": \"get_weather\", \"arguments\": \"...\"}]\n",
      "    )\n",
      "]\n",
      "ground_truth_correct = {\n",
      "    \"tool_calls\": [{\"name\": \"get_weather\", \"arguments\": {...}] \n",
      "}\n",
      "# Score=1.0 (match)\n",
      "```\n",
      "\n",
      "```python\n",
      "# Incorrect call\n",
      "test_messages_incorrect = [\n",
      "    Message(role=\"assistant\", tool_calls=[{\"name\": \"get_current_time\", ...}])\n",
      "]\n",
      "# Score=0.0 (no match)\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Concepts**\n",
      "- **Input Flow**: `messages[-1]` ‚ûù tool_call extraction ‚ûù comparison with ground_truth\n",
      "- **Tool Call Parsing**: Handles both direct `tool_calls` and legacy `<tool_call>` delimiters\n",
      "- **Strict Matching**: Uses `compare_tool_calls()` to check for exact JSON equality\n",
      "\n",
      "This implementation shows how the decorator pattern enables testable, portable reward function logic that works consistently across CLI tools and deployment.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Understanding Reward Functions\n",
    "response = client.responses.create(\n",
    "    model=\"accounts/fireworks/models/qwen3-235b-a22b\",\n",
    "    input=\"How do reward functions work in fw-ai-external/reward-kit? Can you explain the @reward_function decorator and show me some examples from the codebase? I'm particularly interested in understanding the exact_tool_match_reward function.\",\n",
    "    tools=[{\n",
    "        \"type\": \"mcp\",\n",
    "        \"server_label\": \"reward_kit_docs\",\n",
    "        \"server_url\": \"https://gitmcp.io/fw-ai-external/reward-kit\"\n",
    "    }],\n",
    ")\n",
    "\n",
    "print(\"üß† Query: How do reward functions work in reward-kit?\")\n",
    "print(\"=\" * 60)\n",
    "print(response.output[-1].content[0].text.split(\"</think>\")[-1])\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üî• Example 4: CLI Usage and Advanced Features\n",
    "\n",
    "Let's explore the command-line interface and advanced features like dataset integration and evaluation pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Query: CLI commands and dataset integration\n",
      "============================================================\n",
      "\n",
      "\n",
      "The **`reward-kit run`** command is used for comprehensive evaluations using datasets like GSM8K. Here are the key examples and configurations:\n",
      "\n",
      "---\n",
      "\n",
      "### **Example: Evaluating with GSM8K Dataset**\n",
      "```bash\n",
      "# Basic evaluation using the default math configuration (GSM8K dataset)\n",
      "reward-kit run --config-name run_math_eval.yaml --config-path examples/math_example/conf\n",
      "```\n",
      "\n",
      "```bash\n",
      "# Example with overridden parameters:\n",
      "reward-kit run --config-name run_math_eval.yaml --config-path examples/math_example/conf \\\n",
      "  generation.model_name=\"accounts/fireworks/models/llama-v3p1-405b-instruct\" \\\n",
      "  evaluation_params.limit_samples=10\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **What These Commands Do**\n",
      "1. **Loads Dataset**:  \n",
      "   Uses `gsm8k` directly from HuggingFace (configured in `run_math_eval.yaml`).\n",
      "2. **Generates Responses**:  \n",
      "   - Uses the specified model (default or overridden via `generation.model_name`).\n",
      "   - Supports providers like Fireworks, TRL, or custom APIs.\n",
      "3. **Evaluates Output**:  \n",
      "   - Applies reward functions defined in the configuration.\n",
      "   - Saves results to a timestamped directory (e.g., `outputs/2024-06-01_12-34-56/math_example_results.jsonl`).\n",
      "4. **Preview Input-Output Pairs**:  \n",
      "   Saves raw prompt/response pairs to `preview_input_output_pairs.jsonl` for reuse with `reward-kit preview`.\n",
      "\n",
      "---\n",
      "\n",
      "### **Configuration Structure**\n",
      "The `run_math_eval.yaml` configuration (from `examples/math_example/conf`) includes:\n",
      "- **Dataset**:  \n",
      "  ```yaml\n",
      "  dataset:\n",
      "    name: gsm8k\n",
      "    split: test\n",
      "  ```\n",
      "- **Prompting**:  \n",
      "  ```yaml\n",
      "  system_prompt: \"Solve the following math problem. Show your work clearly. Put the final numerical answer between <answer> and </answer> tags.\"\n",
      "  ```\n",
      "- **Model Settings**:  \n",
      "  ```yaml\n",
      "  generation:\n",
      "    model_name: \"accounts/fireworks/models/llama-v3-8b-instruct\"\n",
      "    max_tokens: 512\n",
      "  ```\n",
      "- **Reward Functions**:  \n",
      "  Specifies custom or built-in metrics (e.g., `exact_tool_match_reward` for GSM8K math solutions).\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Features**\n",
      "- **Hydra Integration**: Override parameters directly in the CLI (e.g., `evaluation_params.limit_samples=10`).\n",
      "- **Flexible Dataset Support**:  \n",
      "  Load datasets from HuggingFace or local JSONL files.\n",
      "- **Result Logging**:  \n",
      "  Stores detailed metrics in JSONL files for analysis.\n",
      "- **Custom Reward Functions**:  \n",
      "  Configure evaluation metrics in the `metrics-folders` parameter.\n",
      "\n",
      "---\n",
      "\n",
      "### **Advanced Usage**\n",
      "To debug or test locally:\n",
      "```bash\n",
      "# Disable generation and evaluate cached responses:\n",
      "reward-kit run --config-path examples/apps_coding_example/conf --config-name run_eval generation.enabled=false\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "For full customization, modify the `*.yaml` files in `examples/math_example/conf/` to define:\n",
      "- Dataset formatting rules\n",
      "- Reward function pipelines\n",
      "- Model provider settings\n",
      "\n",
      "Would you like to explore specific reward function implementations or dataset configuration details?\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 4: CLI Commands and Dataset Integration\n",
    "response = client.responses.create(\n",
    "    model=\"accounts/fireworks/models/qwen3-235b-a22b\",\n",
    "    input=\"What CLI commands are available in fw-ai-external/reward-kit? I'm particularly interested in 'reward-kit run' and how to evaluate models with datasets like GSM8K. Can you provide examples of command usage and configuration?\",\n",
    "    tools=[{\n",
    "        \"type\": \"mcp\",\n",
    "        \"server_label\": \"reward_kit_docs\",\n",
    "        \"server_url\": \"https://gitmcp.io/fw-ai-external/reward-kit\"\n",
    "    }],\n",
    ")\n",
    "\n",
    "print(\"‚ö° Query: CLI commands and dataset integration\")\n",
    "print(\"=\" * 60)\n",
    "print(response.output[-1].content[0].text.split(\"</think>\")[-1])\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üî• Example 5: Building Your Own Custom Evaluator\n",
    "\n",
    "Let's create a practical example where we use the insights from the reward-kit repository to build a custom evaluator for tool calling scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Understanding Reward Function Architecture\n",
      "============================================================\n",
      "\n",
      "\n",
      "I'll help you create a custom reward function for tool calling scenarios. Here's a template based on the reward-kit implementation:\n",
      "\n",
      "### 1. Function Structure & Signature\n",
      "```python\n",
      "from reward_kit import EvaluateResult, reward_function\n",
      "from reward_kit.models import Message\n",
      "\n",
      "@reward_function\n",
      "def custom_tool_match_reward(\n",
      "    messages: List[Message],\n",
      "    ground_truth: Optional[Dict[str, Any]] = None,\n",
      "    **kwargs\n",
      ") -> EvaluateResult:\n",
      "    \"\"\"\n",
      "    Evaluate tool calls in messages against ground truth\n",
      "    \n",
      "    Args:\n",
      "        messages: List of conversation messages including tool calls\n",
      "        ground_truth: Expected tool calls in the format:\n",
      "            {\"tool_calls\": [{\"name\": \"tool_name\", \"arguments\": {...}]}\n",
      "        **kwargs: Additional parameters\n",
      "    \n",
      "    Returns:\n",
      "        EvaluateResult with score and metrics\n",
      "    \"\"\"\n",
      "    if not messages:\n",
      "        return EvaluateResult(\n",
      "            score=0.0,\n",
      "            reason=\"No messages provided\",\n",
      "            metrics={}\n",
      "        )\n",
      "    \n",
      "    # Implementation below...\n",
      "```\n",
      "\n",
      "### 2. Handling Tool Calls in Messages\n",
      "```python\n",
      "    # Extract last message\n",
      "    last_msg = messages[-1]\n",
      "    \n",
      "    # Handle different message formats\n",
      "    if hasattr(last_msg, \"tool_calls\"):\n",
      "        # Message object format\n",
      "        generated_calls = [tc.model_dump() for tc in getattr(last_msg, \"tool_calls\", [])]\n",
      "    elif isinstance(last_msg, dict):\n",
      "        # Dictionary format\n",
      "        generated_calls = last_msg.get(\"tool_calls\", [])\n",
      "    else:\n",
      "        return EvaluateResult(\n",
      "            score=0.0,\n",
      "            reason=\"Invalid message format\",\n",
      "            metrics={}\n",
      "        )\n",
      "```\n",
      "\n",
      "### 3. Scoring Best Practices\n",
      "```python\n",
      "    # Default scoring if no ground truth\n",
      "    if not ground_truth:\n",
      "        return EvaluateResult(\n",
      "            score=0.5,\n",
      "            reason=\"No ground truth available\",\n",
      "            metrics={\n",
      "                \"no_ground_truth\": MetricResult(\n",
      "                    score=0.5,\n",
      "                    success=False,\n",
      "                    reason=\"Using default score when ground truth missing\"\n",
      "                )\n",
      "            }\n",
      "        )\n",
      "    \n",
      "    # Get expected tool calls\n",
      "    expected_calls = ground_truth.get(\"tool_calls\", [])\n",
      "    \n",
      "    # Custom comparison logic here (e.g., exact match, parameter validation, etc.)\n",
      "    match_count = sum(\n",
      "        1 for call in generated_calls\n",
      "        if any(\n",
      "            call[\"name\"] == expected[\"name\"] and\n",
      "            call[\"arguments\"] == expected[\"arguments\"]\n",
      "            for expected in expected_calls\n",
      "        )\n",
      "    )\n",
      "    \n",
      "    # Calculate score (0-1 scale)\n",
      "    score = match_count / max(len(expected_calls), 1)\n",
      "    success = score >= 0.9  # Considered successful if ‚â•90% match\n",
      "    \n",
      "    return EvaluateResult(\n",
      "        score=score,\n",
      "        reason=f\"Matched {match_count}/{len(expected_calls)} tool calls\",\n",
      "        metrics={\n",
      "            \"tool_call_match\": MetricResult(\n",
      "                score=score,\n",
      "                success=success,\n",
      "                reason=f\"{'Complete' if success else 'Partial/No'} match\"\n",
      "            )\n",
      "        }\n",
      "    )\n",
      "```\n",
      "\n",
      "### 4. Example Usage Pattern\n",
      "The documentation shows this pattern:\n",
      "```python\n",
      "from reward_kit.rewards.function_calling import exact_tool_match_re\n",
      "from reward_kit.models import Message\n",
      "\n",
      "# Example usage\n",
      "messages = [\n",
      "    Message(\n",
      "        role=\"assistant\", \n",
      "        content=\"\",\n",
      "        tool_calls=[\n",
      "            {\"name\": \"get_weather\", \"arguments\": {\"location\": \"San Francisco\"}}\n",
      "        ]\n",
      "    )\n",
      "]\n",
      "\n",
      "gt = {\n",
      "    \"tool_calls\": [\n",
      "        {\"name\": \"get_weather\", \"arguments\": {\"location\": \"San Francisco\"}}\n",
      "    ]\n",
      "}\n",
      "\n",
      "result = custom_tool_match_reward(messages=messages, ground_truth=gt)\n",
      "print(f\"Score: {result.score}\")  # Should print: Score: 1.0\n",
      "```\n",
      "\n",
      "### Key Implementation Notes:\n",
      "1. **Structure** All reward functions must:\n",
      "   - Be decorated with `@reward_function`\n",
      "   - Accept `messages` and `ground_truth` parameters\n",
      "   - Return an `EvaluateResult` object\n",
      "\n",
      "2. **Tool Call Handling**:\n",
      "   - Use `tool_calls` property from Message objects or dicts\n",
      "   - Support both `Message` and dictionary formats for compatibility\n",
      "\n",
      "3. **MetricResult Best Practices**:\n",
      "   - Return normalized scores (0.0-1.0) for comparability\n",
      "   - Include a `reason` for transparency\n",
      "   - Use hierarchical metrics in the `metrics` dictionary\n",
      "\n",
      "4. **Development Workflow**:\n",
      "   - Test locally with sample JSONL files\n",
      "   - Use `reward-kit preview` command for validation\n",
      "   - Deploy using `reward-kit deploy` when ready\n",
      "\n",
      "For actual implementation details, you may want to clone the repository directly since direct blob accessing might be restricted:\n",
      "```bash\n",
      "git clone https://github.com/fw-ai-external/reward-kit.git\n",
      "cd reward-kit\n",
      "# Check these key locations:\n",
      "# - reward_kit/rewards/function_calling.py for tool call implementation\n",
      "# - examples/tool_calling_example for working examples\n",
      "```\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Let's ask the GitMCP to help us understand the exact structure and implementation \n",
    "# details of reward functions, specifically for tool calling evaluation\n",
    "response = client.responses.create(\n",
    "    model=\"accounts/fireworks/models/qwen3-235b-a22b\",\n",
    "    input=\"\"\"I want to create a custom reward function for evaluating LLM tool calling scenarios with repo fw-ai-external/reward-kit.\n",
    "    Please help me understand:\n",
    "    \n",
    "    1. The exact structure and signature required for reward functions\n",
    "    2. How to properly handle tool_calls in the messages parameter\n",
    "    3. Best practices for scoring and creating MetricResult objects\n",
    "    4. Examples of real reward functions from the codebase that I can adapt\n",
    "    \n",
    "    Show me specific code examples I can use as templates.\"\"\",\n",
    "    tools=[{\n",
    "        \"type\": \"mcp\",\n",
    "        \"server_label\": \"reward_kit_docs\", \n",
    "        \"server_url\": \"https://gitmcp.io/fw-ai-external/reward-kit\"\n",
    "    }],\n",
    ")\n",
    "\n",
    "print(\"üîß Understanding Reward Function Architecture\")\n",
    "print(\"=\" * 60)\n",
    "print(response.output[-1].content[0].text.split(\"</think>\")[-1])\n",
    "print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
