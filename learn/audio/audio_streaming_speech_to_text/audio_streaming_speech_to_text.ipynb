{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d2a32163",
      "metadata": {},
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1arL7bWuF2P3soS3p19MWJeUDtW0Eu5tk?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71a43144",
      "metadata": {
        "id": "71a43144"
      },
      "source": [
        "# Get Started with Streaming Speech-to-Text\n",
        "\n",
        "In this notebook, you will learn how to:\n",
        "\n",
        "*  Open a websocket connection to the Fireworks Streaming Speech-to-Text API;\n",
        "*  Stream audio to the API;\n",
        "*  Receive the transcription of the audio stream;\n",
        "\n",
        "Please note that, the audio stream must be 16000 Hz mono audio chunks representing intervals of 50ms or greater."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c8f3392",
      "metadata": {},
      "source": [
        "## Install dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdd35b36",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install requests torch torchaudio websocket-client"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ced89a3",
      "metadata": {},
      "source": [
        "## 1. Prepare audio stream\n",
        "\n",
        "In this example, we will use a pre-recorded audio file and convert it to a stream of audio chunks each 50ms long."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YYRdRzHuHkjz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYRdRzHuHkjz",
        "outputId": "c497ab98-5b58-44cc-e1ef-3a67c65a4e7e"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import requests\n",
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "# Download audio file\n",
        "response = requests.get(\"https://storage.googleapis.com/fireworks-public/test/3.5m.flac\")\n",
        "audio_bytes = response.content\n",
        "print(f\"Downloaded audio file size: {len(audio_bytes)} bytes\")\n",
        "\n",
        "# Load to torch tensor\n",
        "audio_tensor, sample_rate = torchaudio.load(io.BytesIO(audio_bytes))\n",
        "print(f\"Loaded audio tensor. shape={audio_tensor.shape} sample_rate={sample_rate}\")\n",
        "\n",
        "# Resample to 16000 Hz\n",
        "target_sample_rate = 16000\n",
        "audio_tensor = torchaudio.functional.resample(audio_tensor, sample_rate, target_sample_rate)\n",
        "print(f\"Resampled audio tensor. shape={audio_tensor.shape} sample_rate={target_sample_rate}\")\n",
        "\n",
        "# Convert to mono\n",
        "audio_tensor = audio_tensor.mean(dim=0, keepdim=True)\n",
        "print(f\"Mono audio tensor. shape={audio_tensor.shape}\")\n",
        "\n",
        "# Split into chunks of 50ms\n",
        "chunk_size_ms = 50\n",
        "audio_chunk_tensors = torch.split(audio_tensor, int(chunk_size_ms * target_sample_rate / 1000), dim=1)\n",
        "print(f\"Split into {len(audio_chunk_tensors)} audio chunks each {chunk_size_ms}ms\")\n",
        "\n",
        "# Convert to bytes\n",
        "audio_chunk_bytes = []\n",
        "for audio_chunk_tensor in audio_chunk_tensors:\n",
        "    audio_chunk_bytes.append((audio_chunk_tensor * 32768.0).to(torch.int16).numpy().tobytes())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sV4u2hezqV4X",
      "metadata": {
        "id": "sV4u2hezqV4X"
      },
      "source": [
        "## 2. Stream audio and get transcription\n",
        "\n",
        "We will store transcription segments in a dict, with keys as segment IDs and values as transcription text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f93fe37",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import threading\n",
        "import time\n",
        "import websocket\n",
        "import urllib.parse\n",
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "The client maintains a state dictionary, starting with an empty\n",
        "dictionary `{}`. When the server sends the first transcription message,\n",
        "it contains a list of segments. Each segment has an `id`and `text`:\n",
        "\n",
        "Server initial message:\n",
        "{\n",
        "    \"segments\": [\n",
        "        {\"id\": \"0\", \"text\": \"This is the first sentence\"},\n",
        "        {\"id\": \"1\", \"text\": \"This is the second sentence\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "Client initial state:\n",
        "{\n",
        "    \"0\": \"This is the first sentence\",\n",
        "    \"1\": \"This is the second sentence\",\n",
        "}\n",
        "\n",
        "When the server sends the next updates to the transcription, the client\n",
        "updates the state dictionary based on the segment `id`:\n",
        "\n",
        "Server continuous message:\n",
        "{\n",
        "    \"segments\": [\n",
        "        {\"id\": \"1\", \"text\": \"This is the second sentence modified\"},\n",
        "        {\"id\": \"2\", \"text\": \"This is the third sentence\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "Client updated state:\n",
        "{\n",
        "    \"0\": \"This is the first sentence\",\n",
        "    \"1\": \"This is the second sentence modified\",   # overwritten\n",
        "    \"2\": \"This is the third sentence\",             # new\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "lock = threading.Lock()\n",
        "segments = {}\n",
        "\n",
        "\n",
        "def on_open(ws):\n",
        "    def stream_audio(ws):\n",
        "        for chunk in audio_chunk_bytes:\n",
        "            ws.send(chunk, opcode=websocket.ABNF.OPCODE_BINARY)\n",
        "            time.sleep(chunk_size_ms / 1000)\n",
        "\n",
        "        final_checkpoint = json.dumps({\"checkpoint_id\": \"final\"})\n",
        "        ws.send(final_checkpoint, opcode=websocket.ABNF.OPCODE_TEXT)\n",
        "\n",
        "    threading.Thread(target=stream_audio, args=(ws,)).start()\n",
        "\n",
        "\n",
        "def on_error(ws, error):\n",
        "    print(f\"Error: {error}\")\n",
        "\n",
        "\n",
        "def on_message(ws, message):\n",
        "    message = json.loads(message)\n",
        "    if message.get(\"checkpoint_id\") == \"final\":\n",
        "        ws.close()\n",
        "        return\n",
        "\n",
        "    updated_segments = {\n",
        "        segment[\"id\"]: segment[\"text\"]\n",
        "        for segment in message[\"segments\"]\n",
        "    }\n",
        "    with lock:\n",
        "        segments.update(updated_segments)\n",
        "        clear_output(wait=True)\n",
        "        print(\"\\n\".join(f\" - {k}: {v}\" for k, v in segments.items()))\n",
        "\n",
        "\n",
        "url = \"ws://audio-streaming.us-virginia-1.direct.fireworks.ai/v1/audio/transcriptions/streaming\"\n",
        "params = urllib.parse.urlencode({\n",
        "    \"language\": \"en\",\n",
        "})\n",
        "ws = websocket.WebSocketApp(\n",
        "    f\"{url}?{params}\",\n",
        "    header={\n",
        "        \"Authorization\": \"<FIREWORKS_API_KEY>\",\n",
        "    },\n",
        "    on_open=on_open,\n",
        "    on_message=on_message,\n",
        "    on_error=on_error,\n",
        ")\n",
        "ws.run_forever()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sDoxKlYwDORC",
      "metadata": {
        "id": "sDoxKlYwDORC"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, you learned how to stream audio to the Streaming Speech-to-Text API and receive the transcription in real-time over a websocket connection.\n",
        "\n",
        "For more information visit [docs.fireworks.ai](https://docs.fireworks.ai/api-reference/audio-streaming-transcriptions).\n",
        "\n",
        "Explore the community or reach out to us in [discord](https://discord.gg/fireworks-ai)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
