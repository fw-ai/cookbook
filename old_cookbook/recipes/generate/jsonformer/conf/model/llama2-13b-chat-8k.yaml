defaults:
  - ../../../../common/conf/model/llama2-13b-chat@_here_

name: llama2-13b-chat-8k

flash_attention: False
torch_dtype: float16

rope_scaling:
  type: dynamic
  factor: 2.0
