defaults:
  - ../../../../common/conf/model/llama2-70b-chat@_here_

name: llama2-70b-chat-qlora

micro_batch_size: 2
batch_size: 128

epochs: 8
learning_rate: 2.e-5
cutoff_len: 2048
lora_r: 4
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules:
  - q_proj
  - k_proj
  - v_proj

gradient_checkpointing: True
flash_attention: False

load_in_4bit: True
torch_dtype: bfloat16
bf16: True

quantization_config:
  load_in_4bit: True
  bnb_4bit_use_double_quant: True
  bnb_4bit_quant_type: nf4
  bnb_4bit_compute_dtype: bfloat16
