{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/drive/1sfOOyG7v2hAh2_nE_sMDT19Wp4lQYdf7?usp=sharing\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "Maxim AI provides comprehensive agent monitoring, evaluation, and observability for your Fireworks AI based applications. With Maximâ€™s one-line integration, you can easily trace and analyse agent interactions, performance metrics, and more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1exhoHVzorTQ"
      },
      "source": [
        "## Install Required Dependencies\n",
        "\n",
        "```bash\n",
        "pip install fireworks-ai maxim-py\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZIYx3IWoeq7"
      },
      "source": [
        "## Set the Environment Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSj0yVmcxeYT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "MAXIM_API_KEY=os.getenv(\"MAXIM_API_KEY\")\n",
        "MAXIM_LOG_REPO_ID=os.getenv(\"MAXIM_REPO_ID\")\n",
        "FIREWORKS_API_KEY=os.getenv(\"FIREWORKS_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgas371moiNj"
      },
      "source": [
        "## Configure Maxim Logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fw84whGjy-9u",
        "outputId": "b4240518-1caf-433e-a375-f30e959019eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2152552063.py:5: DeprecationWarning: This class will be removed in a future version. Use {} which is TypedDict.\n",
            "  maxim = Maxim(Config(api_key=os.getenv(\"MAXIM_API_KEY\")))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[MaximSDK] Initializing Maxim AI(v3.10.0)\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2152552063.py:6: DeprecationWarning: This class will be removed in a future version. Use LoggerConfigDict instead.\n",
            "  logger = maxim.logger(LoggerConfig(id=os.getenv(\"MAXIM_LOG_REPO_ID\")))\n"
          ]
        }
      ],
      "source": [
        "from maxim import Config, Maxim\n",
        "from maxim.logger import LoggerConfig\n",
        "\n",
        "maxim = Maxim(Config(api_key=MAXIM_API_KEY))\n",
        "logger = maxim.logger(LoggerConfig(id=MAXIM_LOG_REPO_ID))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hYMbGWA_0Osy"
      },
      "outputs": [],
      "source": [
        "from fireworks import LLM\n",
        "from maxim.logger.fireworks import instrument_fireworks\n",
        "\n",
        "instrument_fireworks(logger)\n",
        "\n",
        "llm = LLM(\n",
        "  model=\"qwen3-235b-a22b\",\n",
        "  deployment_type=\"serverless\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QCZ51BuTpyu"
      },
      "source": [
        "## Simple Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "VvUu4KIbTq8q",
        "outputId": "9c3e6b46-734f-4bce-f090-2bdcc3b33017"
      },
      "outputs": [],
      "source": [
        "response = llm.chat.completions.create(\n",
        "  messages=[{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"Say this is a test\",\n",
        "  }],\n",
        ")\n",
        "\n",
        "response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnDrSiSMUZ16"
      },
      "source": [
        "## Streaming Responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au0YbJ5qTu5U",
        "outputId": "0acff5b9-8ff5-4436-98c7-aa2f0dfae3e5"
      },
      "outputs": [],
      "source": [
        "llm = LLM(\n",
        "  model=\"qwen3-235b-a22b\",\n",
        "  deployment_type=\"serverless\"\n",
        ")\n",
        "response_generator = llm.chat.completions.create(\n",
        "  messages=[{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"Say this is a test\",\n",
        "  }],\n",
        "  stream=True,\n",
        ")\n",
        "for chunk in response_generator:\n",
        "    if chunk.choices[0].delta.content:\n",
        "        print(chunk.choices[0].delta.content, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsOOwFAroSho"
      },
      "source": [
        "## Tool Calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNKVG5Z7VUKz",
        "outputId": "db8ff126-ba86-4614-ee65-65ef84025524"
      },
      "outputs": [],
      "source": [
        "\n",
        "import json\n",
        "\n",
        "llm = LLM(\n",
        "    model=\"llama-v3p1-405b-instruct\",\n",
        "    deployment_type=\"serverless\"\n",
        ")\n",
        "\n",
        "# Define the function tool for getting city population\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            # The name of the function\n",
        "            \"name\": \"get_city_population\",\n",
        "            # A detailed description of what the function does\n",
        "            \"description\": \"Retrieve the current population data for a specified city.\",\n",
        "            # Define the JSON schema for the function parameters\n",
        "            \"parameters\": {\n",
        "                # Always declare a top-level object for parameters\n",
        "                \"type\": \"object\",\n",
        "                # Properties define the arguments for the function\n",
        "                \"properties\": {\n",
        "                    \"city_name\": {\n",
        "                        # JSON Schema type\n",
        "                        \"type\": \"string\",\n",
        "                        # A detailed description of the property\n",
        "                        \"description\": \"The name of the city for which population data is needed, e.g., 'San Francisco'.\"\n",
        "                    },\n",
        "                },\n",
        "                # Specify which properties are required\n",
        "                \"required\": [\"city_name\"],\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "# Define a comprehensive system prompt\n",
        "prompt = f\"\"\"\n",
        "You have access to the following function:\n",
        "\n",
        "Function Name: '{tools[0][\"function\"][\"name\"]}'\n",
        "Purpose: '{tools[0][\"function\"][\"description\"]}'\n",
        "Parameters Schema: {json.dumps(tools[0][\"function\"][\"parameters\"], indent=4)}\n",
        "\n",
        "Instructions for Using Functions:\n",
        "1. Use the function '{tools[0][\"function\"][\"name\"]}' to retrieve population data when required.\n",
        "2. If a function call is necessary, reply ONLY in the following format:\n",
        "   <function={tools[0][\"function\"][\"name\"]}>{{\"city_name\": \"example_city\"}}</function>\n",
        "3. Adhere strictly to the parameters schema. Ensure all required fields are provided.\n",
        "4. Use the function only when you cannot directly answer using general knowledge.\n",
        "5. If no function is necessary, respond to the query directly without mentioning the function.\n",
        "\n",
        "Examples:\n",
        "- For a query like \"What is the population of Toronto?\" respond with:\n",
        "  <function=get_city_population>{{\"city_name\": \"Toronto\"}}</function>\n",
        "- For \"What is the population of the Earth?\" respond with general knowledge and do NOT use the function.\n",
        "\"\"\"\n",
        "\n",
        "# Initial message context\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": prompt},\n",
        "    {\"role\": \"user\", \"content\": \"What is the population of San Francisco?\"}\n",
        "]\n",
        "\n",
        "# Call the model\n",
        "chat_completion = llm.chat.completions.create(\n",
        "    messages=messages,\n",
        "    tools=tools,\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "# Print the model's response\n",
        "print(chat_completion.choices[0].message.model_dump_json(indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgmnA5V0pFHv"
      },
      "source": [
        "## Using JSON Mode\n",
        "\n",
        "```bash\n",
        "pip install pydantic\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "h91pkHqtpKOM"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the Result Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Dpy9CLp8pPge"
      },
      "outputs": [],
      "source": [
        "class Result(BaseModel):\n",
        "    winner: str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9rxs8KUpSNu"
      },
      "source": [
        "### Define Output Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YI2okDSrpQEU"
      },
      "outputs": [],
      "source": [
        "chat_completion = llm.chat.completions.create(\n",
        "    response_format={\n",
        "        \"type\": \"json_schema\",\n",
        "        \"json_schema\": {\n",
        "            \"name\": \"Result\",\n",
        "            \"schema\": Result.model_json_schema()\n",
        "        }\n",
        "    },\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Who won the US presidential election in 2012? Reply just in one JSON.\",\n",
        "        },\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFY-pfv2pcFr",
        "outputId": "1963c3d8-e2aa-4778-9685-90600b7d4a96"
      },
      "outputs": [],
      "source": [
        "print(repr(chat_completion.choices[0].message.content))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
