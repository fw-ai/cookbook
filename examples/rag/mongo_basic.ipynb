{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie recommender example with Fireworks + MongoDB + Mistral E5 embedding model\n",
    "\n",
    "## Introduction\n",
    "In this tutorial, we'll explore how to create an advanced movie recommendation system. We'll leverage the Fireworks API for embedding generation, MongoDB for data storage and retrieval, and the Mistral E5 embedding model for nuanced understanding of movie data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Your Environment\n",
    "Before we dive into the code, make sure to set up your environment. This involves installing necessary packages like pymongo and openai. Run the following command in your notebook to install these packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in /home/bchen/cookbook/.venv/lib/python3.10/site-packages (4.6.1)\n",
      "Requirement already satisfied: openai in /home/bchen/cookbook/.venv/lib/python3.10/site-packages (1.9.0)\n",
      "Requirement already satisfied: tqdm in /home/bchen/cookbook/.venv/lib/python3.10/site-packages (4.66.1)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /home/bchen/cookbook/.venv/lib/python3.10/site-packages (from pymongo) (2.5.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/bchen/cookbook/.venv/lib/python3.10/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/bchen/cookbook/.venv/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/bchen/cookbook/.venv/lib/python3.10/site-packages (from openai) (0.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/bchen/cookbook/.venv/lib/python3.10/site-packages (from openai) (2.5.3)\n",
      "Requirement already satisfied: sniffio in /home/bchen/cookbook/.venv/lib/python3.10/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/bchen/cookbook/.venv/lib/python3.10/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/bchen/cookbook/.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/bchen/cookbook/.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /home/bchen/cookbook/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /home/bchen/cookbook/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/bchen/cookbook/.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/bchen/cookbook/.venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /home/bchen/cookbook/.venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo openai tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Fireworks and MongoDB Clients\n",
    "To interact with Fireworks and MongoDB, we need to initialize their respective clients. Replace \"YOUR FIREWORKS API KEY\" and \"YOUR MONGO URL\" with your actual credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pymongo\n",
    "\n",
    "fw_client = openai.OpenAI(\n",
    "  api_key=\"YOUR FIREWORKS API KEY\",\n",
    "  base_url=\"https://api.fireworks.ai/inference/v1\"\n",
    ")\n",
    "\n",
    "client = pymongo.MongoClient(\"YOUR MONGO URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the E5 Mistral Model\n",
    "\n",
    "The E5 Mistral model, specifically the `intfloat/e5-mistral-7b-instruct` variant, is a highly adaptable language model designed to enhance text embeddings. It has 32 layers and an embedding size of 4096, making it well-suited for complex embedding tasks. It is currently the **state-of-the-art** model on huggingface leaderboard.\n",
    "\n",
    "### Dynamic Adaptation with Instructions\n",
    "A unique feature of E5 Mistral is its ability to adapt to different tasks through natural language instructions in queries. This allows the model to be customized for various scenarios without needing separate models or extensive retraining.\n",
    "\n",
    "### Specialization in English\n",
    "While E5 Mistral is fine-tuned on multilingual datasets, it is primarily recommended for English due to its predominant training on English data. This makes it particularly effective for English-language tasks, such as our movie recommendation system.\n",
    "\n",
    "### Application in Movie Recommendations\n",
    "In our system, we use E5 Mistral's capability to understand context through one-sentence instructions in queries. This feature enables us to generate more accurate and contextually relevant embeddings for movie recommendations.\n",
    "\n",
    "For more details, you can refer to the [E5 Mistral model page on Hugging Face](https://huggingface.co/intfloat/e5-mistral-7b-instruct).\n",
    "\n",
    "## Embedding Generation Function\n",
    "The core of our recommender system is embedding generation. We'll use the Mistral E5 model to create embeddings from text data. The function generate_embeddings takes a list of texts and returns dimensionality-reduced embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# we will need to do pairwise average for the elements to reduce the dimensionality\n",
    "# from 4k to 2k while fitting into MongoDB\n",
    "\n",
    "def generate_embeddings(input_texts: List[str], model_api_string: str, prefix=\"\") -> List[List[float]]:\n",
    "    \"\"\"Generate embeddings from Together python library and reduce their size by averaging adjacent elements.\n",
    "\n",
    "    Args:\n",
    "        input_texts: a list of string input texts.\n",
    "        model_api_string: str. An API string for a specific embedding model of your choice.\n",
    "\n",
    "    Returns:\n",
    "        reduced_embeddings_list: a list of reduced-size embeddings. Each element corresponds to each input text.\n",
    "    \"\"\"\n",
    "    if prefix:\n",
    "        input_texts = [prefix + text for text in input_texts] \n",
    "        print(\"show updated input texts\", input_texts)\n",
    "    outputs = fw_client.embeddings.create(\n",
    "        input=input_texts,\n",
    "        model=model_api_string,\n",
    "    )\n",
    "\n",
    "    def reduce_embedding_size(embedding: List[float]) -> List[float]:\n",
    "        # Average every adjacent pair of elements in the embedding\n",
    "        return [(embedding[i] + embedding[i + 1]) / 2 for i in range(0, len(embedding), 2)]\n",
    "\n",
    "    # Apply the size reduction to each embedding\n",
    "    reduced_embeddings_list = [reduce_embedding_size(x.embedding) for x in outputs.data]\n",
    "\n",
    "    return reduced_embeddings_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "Now, let's process our movie data. We'll extract key information from our MongoDB collection and generate embeddings for each movie. Ensure NUM_DOC_LIMIT is set to limit the number of documents processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size is: 2048\n"
     ]
    }
   ],
   "source": [
    "embedding_model_string = 'intfloat/e5-mistral-7b-instruct' # model API string from Together.\n",
    "vector_database_field_name = 'embedding_2k_movies_fw_e5_mistral' # define your embedding field name.\n",
    "NUM_DOC_LIMIT = 400 # the number of documents you will process and generate embeddings.\n",
    "\n",
    "sample_output = generate_embeddings([\"This is a test.\"], embedding_model_string)\n",
    "print(f\"Embedding size is: {str(len(sample_output[0]))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Document Processing : 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Document Processing : 400it [00:14, 28.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "db = client.sample_mflix\n",
    "collection = db.movies\n",
    "\n",
    "keys_to_extract = [\"plot\", \"genre\", \"cast\", \"title\", \"fullplot\", \"countries\", \"directors\"]\n",
    "for doc in tqdm(collection.find(\n",
    "  {\n",
    "    \"fullplot\":{\"$exists\": True},\n",
    "    \"released\": { \"$gt\": datetime(2000, 1, 1, 0, 0, 0)},\n",
    "  }\n",
    ").limit(NUM_DOC_LIMIT), desc=\"Document Processing \"):\n",
    "  extracted_str = \"\\n\".join([k + \": \" + str(doc[k]) for k in keys_to_extract if k in doc])\n",
    "  if vector_database_field_name not in doc:\n",
    "    doc[vector_database_field_name] = generate_embeddings([extracted_str], embedding_model_string)[0]\n",
    "  collection.replace_one({'_id': doc['_id']}, doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Search Index\n",
    "For our system to efficiently search through movie embeddings, we need to set up a search index in MongoDB. Define the index structure as shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n{\\n  \"fields\": [\\n    {\\n      \"type\": \"vector\",\\n      \"path\": \"embedding_2k_movies_fw_e5_mistral\",\\n      \"numDimensions\": 2048,\\n      \"similarity\": \"dotProduct\"\\n    }\\n  ]\\n}\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "{\n",
    "  \"fields\": [\n",
    "    {\n",
    "      \"type\": \"vector\",\n",
    "      \"path\": \"embedding_2k_movies_fw_e5_mistral\",\n",
    "      \"numDimensions\": 2048,\n",
    "      \"similarity\": \"dotProduct\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the Recommender System\n",
    "Let's test our recommender system. We create a query for superhero movies and exclude Spider-Man movies, as per user preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show updated input texts ['Instruct: Given a user query for movies, retrieve the relevant movie that can fulfill the query.\\nQuery: I love superhero movies, any recommendations?']\n",
      "From your query \"I love superhero movies, any recommendations?\", the following movie listings were found:\n",
      "\n",
      "1. Spider-Man\n",
      "2. Fantastic Four\n",
      "3. Iron Monkey\n",
      "4. Gladiator\n",
      "5. X-Men\n",
      "6. Final Fantasy: The Spirits Within\n",
      "7. Akira\n",
      "8. Crouching Tiger, Hidden Dragon\n",
      "9. Titan A.E.\n"
     ]
    }
   ],
   "source": [
    "# Example query.\n",
    "query = \"I love superhero movies, any recommendations?\"\n",
    "prefix=\"Instruct: Given a user query for movies, retrieve the relevant movie that can fulfill the query.\\nQuery: \"\n",
    "query_emb = generate_embeddings([query], embedding_model_string, prefix=prefix)[0]\n",
    "\n",
    "results = collection.aggregate([\n",
    "  {\n",
    "    \"$vectorSearch\": {\n",
    "      \"queryVector\": query_emb,\n",
    "      \"path\": vector_database_field_name,\n",
    "      \"numCandidates\": 100, # this should be 10-20x the limit\n",
    "      \"limit\": 10, # the number of documents to return in the results\n",
    "      \"index\": \"movie_index\", # the index name you used in Step 4.\n",
    "    }\n",
    "  }\n",
    "])\n",
    "results_as_dict = {doc['title']: doc for doc in results}\n",
    "\n",
    "print(f\"From your query \\\"{query}\\\", the following movie listings were found:\\n\")\n",
    "print(\"\\n\".join([str(i+1) + \". \" + name for (i, name) in enumerate(results_as_dict.keys())]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Recommendations\n",
    "Finally, we use Fireworks' chat API to generate a personalized movie recommendation based on the user's query and preferences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_task_prompt = (\n",
    "    \"From the given movie listing data, choose a great movie recommendation for superhero movies. \"\n",
    "    \"I don't like spider man though. \"\n",
    "    \"Tell me the name of the movie and why it works for me.\"\n",
    ")\n",
    "\n",
    "listing_data = \"\"\n",
    "for doc in results_as_dict.values():\n",
    "  listing_data += f\"Movie title: {doc['title']}\\n\"\n",
    "  for (k, v) in doc.items():\n",
    "    if not(k in keys_to_extract) or (\"embedding\" in k): continue\n",
    "    if k == \"name\": continue\n",
    "    listing_data += k + \": \" + str(v) + \"\\n\"\n",
    "  listing_data += \"\\n\"\n",
    "\n",
    "augmented_prompt = (\n",
    "    \"movie listing data:\\n\"\n",
    "    f\"{listing_data}\\n\\n\"\n",
    "    f\"{your_task_prompt}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on your preference to exclude Spider-Man movies and my commitment to provide a helpful, secure, and positive response, I recommend \"X-Men.\" This movie combines action, drama, and science fiction elements, creating an engaging superhero story.\n",
      "\n",
      "\"X-Men\" features a variety of characters with unique superpowers, which adds excitement and unpredictability to the plot. The central conflict between mutants and humans, along with the internal struggles within the mutant community, offers depth and intrigue.\n",
      "\n",
      "Additionally, the movie has a strong cast, including Hugh Jackman, Patrick Stewart, and Ian McKellen, ensuring high-quality performances. \"X-Men\" is a fantastic choice for those who enjoy superhero movies without focusing on Spider-Man.\n"
     ]
    }
   ],
   "source": [
    "response = fw_client.chat.completions.create(\n",
    "  messages=[{\"role\": \"user\", \"content\": augmented_prompt}],\n",
    "  model=\"accounts/fireworks/models/mixtral-8x7b-instruct\",\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "And that's it! You've successfully built a movie recommendation system using Fireworks, MongoDB, and the Mistral E5 embedding model. This system can be further customized and scaled to suit various needs.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
