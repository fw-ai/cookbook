{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fw-ai/cookbook/blob/main/examples/function_calling/fireworks_langchain_tool_usage.ipynb)"
      ],
      "metadata": {
        "id": "XR-OOx5vQbsA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s40suaVseAN6",
        "outputId": "8e535339-b1e4-404f-cf9d-99bd5b9037cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.5)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.11.1)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.10/dist-packages (0.0.5)\n",
            "Collecting langchainhub\n",
            "  Downloading langchainhub-0.1.14-py3-none-any.whl (3.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.17)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.16 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.18)\n",
            "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.86)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.14)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.26.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: tiktoken<0.6.0,>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.5.2)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
            "  Downloading types_requests-2.31.0.20240125-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<0.6.0,>=0.5.2->langchain_openai) (2023.12.25)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Installing collected packages: types-requests, langchainhub\n",
            "Successfully installed langchainhub-0.1.14 types-requests-2.31.0.20240125\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain openai langchain_openai langchainhub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction - Fireworks x Langchain\n",
        "\n",
        "In this notebook, we demonstrate how we can use Fireworks Function Calling model as a router across multiple models with specialized capabilities. Function calling models have seen rapid rise in usage because of their abilities to use external tools easily. One such powerful tool is other LLMs. We have quite a number of specialized OSS LLMs for [Coding](https://www.deepseek.com/), [Chatting in Certain Language](https://github.com/QwenLM/Qwen) or just plain [HF Assistants](https://huggingface.co/chat/assistants).\n",
        "\n",
        "Function Calling model allows us to\n",
        "1. Analyze the user query for intent\n",
        "2. Find the best model to answer the request. And it could be the FC model itself!\n",
        "3. Construct the right query for the choosen LLM.\n",
        "4. Profit!\n",
        "\n",
        "For this notebook we are going to use [LangChain](https://www.langchain.com/) framework to construct an agent chain which is capable of chit chatting & solving math equations using a calculator tool.\n",
        "\n",
        "This agent chain will take in [custom defined tools](https://python.langchain.com/docs/modules/agents/tools/custom_tools) which are capable of executing a Math Query using a LLM. The main routing LLM is going to be Fireworks function calling model."
      ],
      "metadata": {
        "id": "HSmb-rejDOnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Deps\n",
        "\n",
        "In order to accomplish the task in this notebook we are goign to import some dependencies from langchain along with LLMMathChain. To read more about what this chain does, check this [documentation](https://api.python.langchain.com/en/latest/chains/langchain.chains.llm_math.base.LLMMathChain.html).\n",
        "\n",
        "For solving our math equations, we are going to use the recently released [Mixtral MoE](https://mistral.ai/news/mixtral-of-experts/). For all of our inferencing needs we are going to use the [Fireworks Inference Service](https://fireworks.ai/models).\n",
        "\n",
        "**NOTE:** It's important to set temperature to 0.0 for the function calling model because we want reliable behaviour in routing."
      ],
      "metadata": {
        "id": "pYRSabqIL35i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from langchain.chains import LLMMathChain\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.agents import AgentType\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "from langchain import hub\n",
        "from langchain.pydantic_v1 import BaseModel, Field\n",
        "from langchain.tools import BaseTool, StructuredTool, tool\n",
        "from typing import Optional, Type\n",
        "\n",
        "# Get the prompt to use - you can modify this!\n",
        "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    base_url=\"https://api.fireworks.ai/inference/v1\",\n",
        "    api_key=os.environ[\"FW_API_KEY\"],\n",
        "    model=\"accounts/fireworks/models/fw-function-call-34b-v0\",\n",
        "    temperature=0.0,\n",
        "    max_tokens=256,\n",
        ")\n",
        "\n",
        "math_llm = ChatOpenAI(\n",
        "    base_url=\"https://api.fireworks.ai/inference/v1\",\n",
        "    api_key=os.environ[\"FW_API_KEY\"],\n",
        "    model=\"accounts/fireworks/models/mixtral-8x7b-instruct\",\n",
        "    temperature=0.0,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "rVndeZE_eMFi",
        "outputId": "7aedc24f-6c4f-4ec7-d6fa-052e95b135b4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'FW_API_KEY'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-a59bb06e95cb>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m llm = ChatOpenAI(\n\u001b[1;32m     17\u001b[0m     \u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"https://api.fireworks.ai/inference/v1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"FW_API_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accounts/fireworks/models/fw-function-call-34b-v0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'FW_API_KEY'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Tools\n",
        "\n",
        "In order to seamlessly use function calling ability of the models, we can use [Custom Tools](https://python.langchain.com/docs/modules/agents/tools/custom_tools) functionality built into LangChain. This allows us to seamlessly define the schema of the functions our model can access along with the execution logic. It simplifies the JSON function specification construction.\n",
        "\n",
        "For this notebook, we are going to construct a `CustomCalculatorTool` which will use LLM to answer & execute math queries. It takes in a single parameter `query` which has to be valid mathemetical expression."
      ],
      "metadata": {
        "id": "EGjCfWemNsse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CalculatorInput(BaseModel):\n",
        "    query: str = Field(description=\"should be a math expression\")\n",
        "\n",
        "class CustomCalculatorTool(BaseTool):\n",
        "    name: str = \"Calculator\"\n",
        "    description: str = \"Tool to evaluate mathemetical expressions\"\n",
        "    args_schema: Type[BaseModel] = CalculatorInput\n",
        "\n",
        "    def _run(self, query: str) -> str:\n",
        "        \"\"\"Use the tool.\"\"\"\n",
        "        return LLMMathChain(llm=math_llm, verbose=True).run(query)\n",
        "\n",
        "    async def _arun(self, query: str) -> str:\n",
        "        \"\"\"Use the tool asynchronously.\"\"\"\n",
        "        raise NotImplementedError(\"not support async\")\n",
        "\n",
        "tools = [\n",
        "  CustomCalculatorTool()\n",
        "]\n",
        "\n",
        "agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "\n",
        "agent = AgentExecutor(agent=agent, tools=tools, verbose=True)"
      ],
      "metadata": {
        "id": "50g_NBE_Atn5"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Chit Chat Ability\n",
        "\n",
        "As we outlined in the beginning, the model should be able to both chit-chat, route queries to external tools when necessary or answer from internal knowledge.\n",
        "\n",
        "Let's first start with a question that can be answered from internal knowledge."
      ],
      "metadata": {
        "id": "n2FKMb9gORy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(agent.invoke({\"input\": \"What is the capital of USA?\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A18JJlKaOgLL",
        "outputId": "5f0ce2fe-b2ad-4404-8394-f825fd123d71"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThe capital of USA is Washington, D.C. \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'input': 'What is the capital of USA?', 'output': 'The capital of USA is Washington, D.C. '}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Calculator\n",
        "\n",
        "Now let's test it's ability to detect a mathemetical question & route the query accordingly."
      ],
      "metadata": {
        "id": "jL0hyeYCOikE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(agent.invoke({\"input\": \"What is 100 devided by 25?\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qu0HFN35OuRo",
        "outputId": "89d47217-4b03-4830-add7-03b8b32b61a2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `Calculator` with `{'query': '100/25'}`\n",
            "\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
            "100/25"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm_math/base.py:57: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m```text\n",
            "100 / 25\n",
            "```\n",
            "...numexpr.evaluate(\"100 / 25\")...\n",
            "\u001b[0m\n",
            "Answer: \u001b[33;1m\u001b[1;3m4.0\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mAnswer: 4.0\u001b[0m\u001b[32;1m\u001b[1;3mThe answer is 4.0. \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'input': 'What is 100 devided by 25?', 'output': 'The answer is 4.0. '}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "The fireworks function calling model is able to route reuqest to external tools or internal knowledge appropriately - thus helping developers build co-operative agents."
      ],
      "metadata": {
        "id": "XFrp6ckCOyYt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uNHOAmqfQT33"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}