{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR-OOx5vQbsA"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fw-ai/cookbook/blob/main/examples/function_calling/fireworks_langchain_tool_usage.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s40suaVseAN6"
      },
      "outputs": [],
      "source": [
        "!pip install langchain openai langchain_openai langchainhub numexpr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSmb-rejDOnT"
      },
      "source": [
        "# Introduction - Fireworks x Langchain\n",
        "\n",
        "In this notebook, we demonstrate how we can use Fireworks Function Calling model as a router across multiple models with specialized capabilities. Function calling models have seen rapid rise in usage because of their abilities to use external tools easily. One such powerful tool is other LLMs. We have quite a number of specialized OSS LLMs for [Coding](https://www.deepseek.com/), [Chatting in Certain Language](https://github.com/QwenLM/Qwen) or just plain [HF Assistants](https://huggingface.co/chat/assistants).\n",
        "\n",
        "Function Calling model allows us to\n",
        "1. Analyze the user query for intent\n",
        "2. Find the best model to answer the request. And it could be the FC model itself!\n",
        "3. Construct the right query for the choosen LLM.\n",
        "4. Profit!\n",
        "\n",
        "For this notebook we are going to use [LangChain](https://www.langchain.com/) framework to construct an agent chain which is capable of chit chatting & solving math equations using a calculator tool.\n",
        "\n",
        "This agent chain will take in [custom defined tools](https://python.langchain.com/docs/modules/agents/tools/custom_tools) which are capable of executing a Math Query using a LLM. The main routing LLM is going to be Fireworks function calling model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYRSabqIL35i"
      },
      "source": [
        "## Setup Deps\n",
        "\n",
        "In order to accomplish the task in this notebook we are goign to import some dependencies from langchain along with LLMMathChain. To read more about what this chain does, check this [documentation](https://api.python.langchain.com/en/latest/chains/langchain.chains.llm_math.base.LLMMathChain.html).\n",
        "\n",
        "For solving our math equations, we are going to use the recently released [Mixtral MoE](https://mistral.ai/news/mixtral-of-experts/). For all of our inferencing needs we are going to use the [Fireworks Inference Service](https://fireworks.ai/models).\n",
        "\n",
        "In order to use the Fireworks AI function calling model, you must first obtain Fireworks API Keys. If you don't already have one, you can one by following the instructions [here](https://readme.fireworks.ai/docs/quickstart). Replace `YOUR_FW_API_KEY` with your obtained key.\n",
        "\n",
        "**NOTE:** It's important to set temperature to 0.0 for the function calling model because we want reliable behaviour in routing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVndeZE_eMFi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from langchain.chains import LLMMathChain\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.agents import AgentType\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "from langchain import hub\n",
        "from langchain.pydantic_v1 import BaseModel, Field\n",
        "from langchain.tools import BaseTool, StructuredTool, tool\n",
        "from typing import Optional, Type\n",
        "\n",
        "# Get the prompt to use - you can modify this!\n",
        "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    base_url=\"https://api.fireworks.ai/inference/v1\",\n",
        "    api_key=\"YOUR_FW_API_KEY\",\n",
        "    model=\"accounts/fireworks/models/firefunction-v1\",\n",
        "    temperature=0.0,\n",
        "    max_tokens=256,\n",
        ")\n",
        "\n",
        "math_llm = ChatOpenAI(\n",
        "    base_url=\"https://api.fireworks.ai/inference/v1\",\n",
        "    api_key=\"YOUR_FW_API_KEY\",\n",
        "    model=\"accounts/fireworks/models/mixtral-8x7b-instruct\",\n",
        "    temperature=0.0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGjCfWemNsse"
      },
      "source": [
        "## Custom Tools\n",
        "\n",
        "In order to seamlessly use function calling ability of the models, we can use [Custom Tools](https://python.langchain.com/docs/modules/agents/tools/custom_tools) functionality built into LangChain. This allows us to seamlessly define the schema of the functions our model can access along with the execution logic. It simplifies the JSON function specification construction.\n",
        "\n",
        "For this notebook, we are going to construct a `CustomCalculatorTool` which will use LLM to answer & execute math queries. It takes in a single parameter `query` which has to be valid mathemetical expression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50g_NBE_Atn5"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CalculatorInput(BaseModel):\n",
        "    query: str = Field(description=\"should be a math expression\")\n",
        "\n",
        "class CustomCalculatorTool(BaseTool):\n",
        "    name: str = \"Calculator\"\n",
        "    description: str = \"Tool to evaluate mathemetical expressions\"\n",
        "    args_schema: Type[BaseModel] = CalculatorInput\n",
        "\n",
        "    def _run(self, query: str) -> str:\n",
        "        \"\"\"Use the tool.\"\"\"\n",
        "        return LLMMathChain(llm=math_llm, verbose=True).run(query)\n",
        "\n",
        "    async def _arun(self, query: str) -> str:\n",
        "        \"\"\"Use the tool asynchronously.\"\"\"\n",
        "        raise NotImplementedError(\"not support async\")\n",
        "\n",
        "tools = [\n",
        "  CustomCalculatorTool()\n",
        "]\n",
        "\n",
        "agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "\n",
        "agent = AgentExecutor(agent=agent, tools=tools, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2FKMb9gORy-"
      },
      "source": [
        "### Test Chit Chat Ability\n",
        "\n",
        "As we outlined in the beginning, the model should be able to both chit-chat, route queries to external tools when necessary or answer from internal knowledge.\n",
        "\n",
        "Let's first start with a question that can be answered from internal knowledge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A18JJlKaOgLL",
        "outputId": "5f0ce2fe-b2ad-4404-8394-f825fd123d71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThe capital of USA is Washington, D.C. \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'input': 'What is the capital of USA?', 'output': 'The capital of USA is Washington, D.C. '}\n"
          ]
        }
      ],
      "source": [
        "print(agent.invoke({\"input\": \"What is the capital of USA?\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jL0hyeYCOikE"
      },
      "source": [
        "## Test Calculator\n",
        "\n",
        "Now let's test it's ability to detect a mathematical question & route the query accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qu0HFN35OuRo",
        "outputId": "89d47217-4b03-4830-add7-03b8b32b61a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `Calculator` with `{'query': '100/25'}`\n",
            "\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
            "100/25"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm_math/base.py:57: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m```text\n",
            "100 / 25\n",
            "```\n",
            "...numexpr.evaluate(\"100 / 25\")...\n",
            "\u001b[0m\n",
            "Answer: \u001b[33;1m\u001b[1;3m4.0\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mAnswer: 4.0\u001b[0m\u001b[32;1m\u001b[1;3mThe answer is 4.0. \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'input': 'What is 100 divided by 25?', 'output': 'The answer is 4.0. '}\n"
          ]
        }
      ],
      "source": [
        "print(agent.invoke({\"input\": \"What is 100 divided by 25?\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFrp6ckCOyYt"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "The fireworks function calling model can route request to external tools or internal knowledge appropriately - thus helping developers build co-operative agents."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
