{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fw-ai/cookbook/blob/main/examples/function_calling/fireworks_functions_qa.ipynb)"
      ],
      "metadata": {
        "id": "f5OOOJoXB-O3"
      },
      "id": "f5OOOJoXB-O3"
    },
    {
      "cell_type": "markdown",
      "id": "71a43144",
      "metadata": {
        "id": "71a43144"
      },
      "source": [
        "# Structured answers with Fireworks functions\n",
        "\n",
        "Several real world applications of LLM require them to respond in a strucutred manner. This structured response could look like `JSON` or `YAML`. For e.g. answering research questions using arxiv along with citations. Instead of parsing the entire LLM response and trying to figure out the actual answer of the LLM vs the citations provided by the LLM, we can use function calling ability of the LLMs to answer questions in a structured way.\n",
        "\n",
        "In this notebook, we demonstrate structured response generation ability of the Fireworks function calling model. We will build an application that can answer questions (along with citations) regarding the State of the Union speech of 2022."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "Install all the dependencies and import the required python modules."
      ],
      "metadata": {
        "id": "-7tAxHrBp4IQ"
      },
      "id": "-7tAxHrBp4IQ"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "h2cmLE1ACL17"
      },
      "id": "h2cmLE1ACL17",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f059012e",
      "metadata": {
        "id": "f059012e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import re\n",
        "import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Download & Clean the Content\n",
        "\n",
        "We are going to download the content using the python package `requests` and perform minor cleanup by removing several newlines. Even minimal cleanup should be good enough to obtain good results with the model."
      ],
      "metadata": {
        "id": "tgbH6j3Lp-_x"
      },
      "id": "tgbH6j3Lp-_x"
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/hwchase17/chat-your-data/master/state_of_the_union.txt\"\n",
        "content = requests.get(url).content\n",
        "content = str(content, \"utf-8\")"
      ],
      "metadata": {
        "id": "IcIybYoE35ro"
      },
      "id": "IcIybYoE35ro",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some clean up\n",
        "clean_content = content.replace(\"\\n\\n\", \"\\n\")"
      ],
      "metadata": {
        "id": "xTeisbO_4UI7"
      },
      "id": "xTeisbO_4UI7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup your API Key\n",
        "\n",
        "In order to use the Fireworks AI function calling model, you must first obtain Fireworks API Keys. If you don't already have one, you can one by following the instructions [here](https://readme.fireworks.ai/docs/quickstart)."
      ],
      "metadata": {
        "id": "XBfEwDuiqQMT"
      },
      "id": "XBfEwDuiqQMT"
    },
    {
      "cell_type": "code",
      "source": [
        "client = openai.OpenAI(\n",
        "    base_url = \"https://api.fireworks.ai/inference/v1\",\n",
        "    api_key = \"YOUR_FW_API_KEY\",\n",
        ")\n",
        "model_name = \"accounts/fireworks/models/firefunction-v1\""
      ],
      "metadata": {
        "id": "ZlTFlhtB5baq"
      },
      "id": "ZlTFlhtB5baq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Structure\n",
        "\n",
        "Let's define the strucutre in which we want our model to responsd. The JSON structure for function calling follows the conventions of [JSON Schema](https://json-schema.org/). Here we define a structure with `answer` and `citations` field."
      ],
      "metadata": {
        "id": "JoHfdVFlqbjN"
      },
      "id": "JoHfdVFlqbjN"
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_answer_with_sources\",\n",
        "            \"description\": \"Answer questions from the user while quoting sources.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                  \"answer\": {\n",
        "                      \"type\": \"string\",\n",
        "                      \"description\": \"Answer to the question that was asked.\"\n",
        "                  },\n",
        "                  \"sources\": {\n",
        "                      \"type\": \"array\",\n",
        "                      \"items\": {\n",
        "                          \"type\": \"string\",\n",
        "                          \"description\": \"Source used to answer the question\"\n",
        "                      }\n",
        "                  }\n",
        "                },\n",
        "                \"required\": [\"answer\", \"sources\"],\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "tool_choice = {\"type\": \"function\", \"function\": {\"name\":\"get_answer_with_sources\"}}"
      ],
      "metadata": {
        "id": "Zj-9l4m283b4"
      },
      "id": "Zj-9l4m283b4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform Sanity Test\n",
        "\n",
        "Let's perform a sanity test by querying the speech for some basic information. This would ensure that our model setup is working correctly and the document is being processed correctly."
      ],
      "metadata": {
        "id": "4tz7bwV-qset"
      },
      "id": "4tz7bwV-qset"
    },
    {
      "cell_type": "code",
      "source": [
        "mp\n",
        "essages = [\n",
        "    {\"role\": \"system\", \"content\": f\"You are a helpful assistant who is given document with following content: {clean_content}.\"\n",
        "     \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"Please reply in succinct manner and be truthful in the reply.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What did the president say about Ketanji Brown Jackson?\"}\n",
        "]\n"
      ],
      "metadata": {
        "id": "LcnDoz7H8jjE"
      },
      "id": "LcnDoz7H8jjE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=messages,\n",
        "    tools=tools,\n",
        "    tool_choice=tool_choice,\n",
        "    temperature=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "ENX3Fgcd_JfZ"
      },
      "id": "ENX3Fgcd_JfZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat_completion.choices[0].message.model_dump_json(indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WzRJ5PgFAXc",
        "outputId": "ee5a8472-167e-4167-f716-94378d8bd333"
      },
      "id": "0WzRJ5PgFAXc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"content\": \" \",\n",
            "    \"role\": \"assistant\",\n",
            "    \"function_call\": null,\n",
            "    \"tool_calls\": [\n",
            "        {\n",
            "            \"id\": \"call_fN2C7m2xaRrkZj0O9gV46eKb\",\n",
            "            \"function\": {\n",
            "                \"arguments\": \"{\\\"answer\\\": \\\"I recently nominated Ketanji Brown Jackson to serve on the United States Supreme Court. She is an outstanding candidate and I am confident that she will serve with distinction.\\\", \\\"sources\\\": [\\\"The White House\\\"]}\",\n",
            "                \"name\": \"get_answer_with_sources\"\n",
            "            },\n",
            "            \"type\": \"function\",\n",
            "            \"index\": 0\n",
            "        }\n",
            "    ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_response = chat_completion.choices[0].message\n",
        "\n",
        "messages.append(\n",
        "    {\n",
        "        \"role\": agent_response.role,\n",
        "        \"content\": \"\",\n",
        "        \"tool_calls\": [\n",
        "            tool_call.model_dump()\n",
        "            for tool_call in agent_response.tool_calls\n",
        "        ]\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "bF-o87oxD05g"
      },
      "id": "bF-o87oxD05g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Function Calling in Conversation\n",
        "\n",
        "Our model currently support multi-turn conversation when using function calling. You can reference previous completions generated by the model to ask more clarifying questions."
      ],
      "metadata": {
        "id": "oC2FSQjDAyL8"
      },
      "id": "oC2FSQjDAyL8"
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append(\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What did he say about her predecessor?\"\n",
        "    }\n",
        ")\n",
        "next_chat_completion = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=messages,\n",
        "    tools=tools,\n",
        "    tool_choice=tool_choice,\n",
        "    temperature=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "wYGPiSXfAysM"
      },
      "id": "wYGPiSXfAysM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(next_chat_completion.choices[0].message.model_dump_json(indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkWhQ4hPFMc_",
        "outputId": "dcce465f-8d89-4937-f17c-4906dc142dcd"
      },
      "id": "UkWhQ4hPFMc_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"content\": null,\n",
            "    \"role\": \"assistant\",\n",
            "    \"function_call\": null,\n",
            "    \"tool_calls\": [\n",
            "        {\n",
            "            \"id\": \"call_SJsaq3GUUK2ugOVlJ025flwH\",\n",
            "            \"function\": {\n",
            "                \"arguments\": \"{\\\"answer\\\": \\\"I am deeply saddened by the passing of Justice Breyer. He was a brilliant jurist and a true American hero. His legacy will live on in the work he did on the Supreme Court and in the lives he touched.\\\", \\\"sources\\\": [\\\"The White House\\\"]}\",\n",
            "                \"name\": \"get_answer_with_sources\"\n",
            "            },\n",
            "            \"type\": \"function\",\n",
            "            \"index\": 0\n",
            "        }\n",
            "    ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modifying the output format to more specific one\n",
        "\n",
        "During the conversation, some questions might need a more flexible response format. We have flexibility to change that during the conversation.\n",
        "\n"
      ],
      "metadata": {
        "id": "EU_U5Bm9GJNa"
      },
      "id": "EU_U5Bm9GJNa"
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_answer_with_countries\",\n",
        "            \"description\": \"Answer questions from the user while quoting sources.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                  \"answer\": {\n",
        "                      \"type\": \"string\",\n",
        "                      \"description\": \"Answer to the question that was asked.\"\n",
        "                  },\n",
        "                  \"countries\": {\n",
        "                      \"type\": \"array\",\n",
        "                      \"items\": {\n",
        "                          \"type\": \"string\",\n",
        "                      },\n",
        "                      \"description\": \"countries mentioned in the sources\"\n",
        "                  }\n",
        "                },\n",
        "                \"required\": [\"answer\", \"countries\"],\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "tool_choice = {\"type\": \"function\", \"function\": {\"name\":\"get_answer_with_countries\"}}"
      ],
      "metadata": {
        "id": "JxsGWpUcIGan"
      },
      "id": "JxsGWpUcIGan",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_response = next_chat_completion.choices[0].message\n",
        "\n",
        "messages.append(\n",
        "    {\n",
        "        \"role\": agent_response.role,\n",
        "        \"content\": \"\",\n",
        "        \"tool_calls\": [\n",
        "            tool_call.model_dump()\n",
        "            for tool_call in agent_response.tool_calls\n",
        "        ]\n",
        "    }\n",
        ")\n",
        "\n",
        "messages.append(\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What did he say about human traffickers?\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "g-CYqzXIIUIl"
      },
      "id": "g-CYqzXIIUIl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=messages,\n",
        "    tools=tools,\n",
        "    tool_choice=tool_choice,\n",
        "    temperature=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "RylJZ8BiIewx"
      },
      "id": "RylJZ8BiIewx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat_completion.choices[0].message.model_dump_json(indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qi_gNf-qI-CG",
        "outputId": "d3018c86-21dd-4b40-9a38-7f14bbec05cd"
      },
      "id": "qi_gNf-qI-CG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"content\": null,\n",
            "    \"role\": \"assistant\",\n",
            "    \"function_call\": null,\n",
            "    \"tool_calls\": [\n",
            "        {\n",
            "            \"id\": \"call_0Qymk6ZFNVsdjXmOOsmMsYLW\",\n",
            "            \"function\": {\n",
            "                \"arguments\": \"{\\\"answer\\\": \\\"We are working with our partners in South and Central America to host more refugees and secure their own borders. This will help us crack down on human traffickers and reduce the flow of illegal immigrants into the United States.\\\", \\\"countries\\\": [\\\"South America\\\", \\\"Central America\\\"]}\",\n",
            "                \"name\": \"get_answer_with_countries\"\n",
            "            },\n",
            "            \"type\": \"function\",\n",
            "            \"index\": 0\n",
            "        }\n",
            "    ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s9RBCjDHKr2Z"
      },
      "id": "s9RBCjDHKr2Z",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}