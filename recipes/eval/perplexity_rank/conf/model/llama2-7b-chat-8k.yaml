defaults:
  - ../../../../common/conf/model/llama2-7b-chat@_here_

client_class: recipes.eval.perplexity_rank.client.LocalClient

name: llama2-7b-chat-8k

flash_attention: False
torch_dtype: float16

rope_scaling:
  type: dynamic
  factor: 2.0
