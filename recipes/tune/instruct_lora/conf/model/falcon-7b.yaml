defaults:
  - ../../../../common/conf/model/falcon-7b@_here_

name: falcon-7b

micro_batch_size: 2
batch_size: 128

epochs: 13
learning_rate: 3.e-5
cutoff_len: 2048
lora_r: 64
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules:
  - query_key_value

gradient_checkpointing: False
flash_attention: False

load_in_4bit: False
torch_dtype: bfloat16
bf16: True
