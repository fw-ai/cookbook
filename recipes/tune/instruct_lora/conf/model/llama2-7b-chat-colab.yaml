defaults:
  - ../../../../common/conf/model/llama2-7b-chat@_here_

name: llama2-7b-chat-colab

micro_batch_size: 2
batch_size: 32

epochs: 1
learning_rate: 2.e-5
cutoff_len: 512
lora_r: 64
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules:
  - q_proj
  - k_proj
  - v_proj

gradient_checkpointing: True
flash_attention: False

load_in_4bit: True
torch_dtype: float16
fp16: True

quantization_config:
  load_in_4bit: True
  bnb_4bit_use_double_quant: True
  bnb_4bit_quant_type: nf4
  bnb_4bit_compute_dtype: float16
