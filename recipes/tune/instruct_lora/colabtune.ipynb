{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmLUG6Yte4VC"
      },
      "source": [
        "# Model tuning in colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBQALK1uzPX5"
      },
      "source": [
        "**Note:** this notebook is intended as a demonstration that finetuning can be done in a free colab tier. If you need to train a large model on a sizable dataset, you should consider upgrading to a pro tier and getting more resources. Otherwise, the training will take impractically long time. If you don't use a large enough model or tune on insufficient number of sampes, the quality won't be good."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_Tbt2vYfIvB"
      },
      "source": [
        "## Environment setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w48IGhuPfOLd"
      },
      "source": [
        "Make sure that you are connected to a GPU host."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1SRQmvFezo1"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn9T_kW3fZOg"
      },
      "source": [
        "Check out the cook book code from github."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usSJUnJve5Kp"
      },
      "outputs": [],
      "source": [
        "!git clone https://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@github.com/fw-ai/cookbook.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q92kngbbgBV6"
      },
      "source": [
        "Install required dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dezn_0k3ffCr"
      },
      "outputs": [],
      "source": [
        "!pip install \\\n",
        "    accelerate \\\n",
        "    bitsandbytes \\\n",
        "    datasets \\\n",
        "    evals \\\n",
        "    fire \\\n",
        "    guidance \\\n",
        "    huggingface_hub \\\n",
        "    hydra-core \\\n",
        "    ninja \\\n",
        "    packaging \\\n",
        "    peft \\\n",
        "    py7zr \\\n",
        "    s3fs \\\n",
        "    sentencepiece \\\n",
        "    torchx \\\n",
        "    transformers \\\n",
        "    zstandard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0niRKJv-hKLq"
      },
      "source": [
        "Log into huggingface. For that, you will need a huggingface token. You can find it by going to https://huggingface.co/settings/tokens\n",
        "If you are running a recipe that uses llama2 models, don't forget to sign the license agreement on the model card."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiA7dxI4gdhD"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login --token <your_hf_token>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBW5n1wegwdQ"
      },
      "outputs": [],
      "source": [
        "!PYTHONPATH=\"$PYTHONPATH:/content/cookbook\" torchx run -s local_cwd dist.ddp -j 1x1 --script /content/cookbook/recipes/tune/instruct_lora/finetune.py -- --config-name=summarize model=llama2-7b-chat-colab"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
