{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv seaborn matplotlib pandas numpy locust==2.18.1 orjson==3.9.10\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import json\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Bench Guide\n",
    "\n",
    "This guide explains how to use the benchmarking tools to evaluate LLM performance.\n",
    "\n",
    "### Metrics Collected\n",
    "The `load_test.py` script measures:\n",
    "\n",
    "1. Average Time to First Token\n",
    "2. Average Token Latency\n",
    "3. Average Token Count\n",
    "4. Average Total Response Time\n",
    "5. Request Count\n",
    "6. Queries Per Second (QPS)\n",
    "7. Latency Percentiles (p50, p90, p99, p99.9)\n",
    "   - Time to First Token\n",
    "   - Total Response Time\n",
    "\n",
    "### Benchmark Scenarios\n",
    "We will create the following benchmark suites in this notebook. Although there are many other types of tests that can be run, please refer to the README.md for more details:\n",
    "\n",
    "1. Single Model Performance Analysis\n",
    "2. Comparing Performance of different Models and Providers\n",
    "3. Token Length Impact Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Helper functions, you can ignore this'''\n",
    "\n",
    "#Function to utilize subprocess to run the locust script\n",
    "def execute_subprocess(cmd):\n",
    "    print(f\"\\nExecuting benchmark: {' '.join(cmd)}\\n\")\n",
    "    process = subprocess.Popen(\n",
    "        cmd,\n",
    "        text=True,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        bufsize=1,\n",
    "        universal_newlines=True\n",
    "    )\n",
    "    # Display output in real-time\n",
    "    while True:\n",
    "        output = process.stdout.readline()\n",
    "        if output == '' and process.poll() is not None:\n",
    "            break\n",
    "        if output:\n",
    "            print(output.strip())\n",
    "\n",
    "    return_code = process.poll()\n",
    "    if return_code != 0:\n",
    "        print(f\"Benchmark failed with return code: {return_code}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "# Optional: for higher resolution plots\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "\n",
    "def visualize_comparative_results(stat_result_paths, results_dir):\n",
    "    \"\"\"\n",
    "    Create comparative visualizations for multiple model benchmark results\n",
    "    \n",
    "    Args:\n",
    "        stat_result_paths (list): List of dicts containing paths to stats files and configs\n",
    "            [{\"path\": \"path/to/stats.csv\", \"config\": {\"provider\": \"...\", \"model\": \"...\"}}, ...]\n",
    "        results_dir (str): Directory to save the comparative visualizations\n",
    "    \"\"\"\n",
    "    # Read and combine all CSV files with model information\n",
    "    dfs = []\n",
    "    for result in stat_result_paths:\n",
    "        df = pd.read_csv(result['path'])\n",
    "        df['model'] = result['config']['model']\n",
    "        df['provider'] = result['config']['provider']\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Get POST data first\n",
    "    post_data = combined_df[combined_df['Type'] == 'POST']\n",
    "    \n",
    "    # Get unique models and calculate dynamic bar width\n",
    "    models = post_data['model'].unique()\n",
    "    num_models = len(models)\n",
    "    bar_width = min(0.35, 0.8 / num_models)  # Dynamically reduce bar width as models increase\n",
    "    \n",
    "    # Set style for better visualizations\n",
    "    plt.style.use('ggplot')\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "    # 1. Response Time Distribution Comparison\n",
    "    plt.subplot(2, 2, 1)\n",
    "    metrics_to_plot = ['Average Response Time', 'Median Response Time']\n",
    "    \n",
    "    x = np.arange(len(metrics_to_plot))\n",
    "    # Adjust bar positions to be centered\n",
    "    positions = np.linspace(-(bar_width * (num_models-1))/2, \n",
    "                          (bar_width * (num_models-1))/2, \n",
    "                          num_models)\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        model_data = post_data[post_data['model'] == model][metrics_to_plot]\n",
    "        plt.bar(x + positions[i], model_data.iloc[0], bar_width, label=model.split('/')[-1])\n",
    "    \n",
    "    plt.title('Response Time Comparison')\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Time (ms)')\n",
    "    plt.xticks(x, metrics_to_plot, rotation=45)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    # 2. QPS Comparison\n",
    "    plt.subplot(2, 2, 2)\n",
    "    qps_data = combined_df[combined_df['Type'] == 'POST'][['model', 'Requests/s']]\n",
    "    x = np.arange(len(models))\n",
    "    plt.bar(x, [qps_data[qps_data['model'] == model]['Requests/s'].iloc[0] for model in models], \n",
    "            width=0.6)  # Single bars can be wider\n",
    "    plt.title('Throughput Comparison')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Requests per Second')\n",
    "    plt.xticks(x, [model.split('/')[-1] for model in models], rotation=45)\n",
    "\n",
    "    # 3. Token Latency Comparison\n",
    "    plt.subplot(2, 2, 3)\n",
    "    token_metrics = ['latency_per_token', 'overall_latency_per_token']\n",
    "    token_data = combined_df[\n",
    "        (combined_df['Type'] == 'METRIC') & \n",
    "        (combined_df['Name'].isin(token_metrics))\n",
    "    ]\n",
    "    \n",
    "    x = np.arange(len(token_metrics))\n",
    "    for i, model in enumerate(models):\n",
    "        model_data = token_data[token_data['model'] == model]\n",
    "        values = [\n",
    "            model_data[model_data['Name'] == metric]['Average Response Time'].iloc[0]\n",
    "            for metric in token_metrics\n",
    "        ]\n",
    "        plt.bar(x + positions[i], values, bar_width, label=model.split('/')[-1])\n",
    "    \n",
    "    plt.title('Token Latency Comparison')\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Time (ms)')\n",
    "    plt.xticks(x, ['Per Token', 'Overall Per Token'], rotation=45)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    # 4. Percentile Distribution Comparison\n",
    "    plt.subplot(2, 2, 4)\n",
    "    percentiles = ['50%', '75%', '90%', '95%', '99%', '99.9%']\n",
    "    \n",
    "    for model in models:\n",
    "        model_data = post_data[post_data['model'] == model]\n",
    "        plt.plot(percentiles, model_data[percentiles].iloc[0], marker='o', label=model.split('/')[-1])\n",
    "    \n",
    "    plt.title('Response Time Percentiles Comparison')\n",
    "    plt.xlabel('Percentile')\n",
    "    plt.ylabel('Response Time (ms)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    # Adjust layout to prevent overlapping\n",
    "    plt.tight_layout()\n",
    "    # Save the figure\n",
    "    plt.savefig(f'{results_dir}/comparative_performance_metrics.png', \n",
    "                bbox_inches='tight',  # Ensure the legend is included in the saved figure\n",
    "                dpi=300)  # Higher resolution\n",
    "    # Display in notebook\n",
    "    plt.show()\n",
    "    # Close the figure\n",
    "    plt.close()\n",
    "\n",
    "    # Generate summary statistics\n",
    "    summary_stats = []\n",
    "    for model in models:\n",
    "        model_data = combined_df[combined_df['model'] == model]\n",
    "        post_data = model_data[model_data['Type'] == 'POST'].iloc[0]\n",
    "        token_data = model_data[model_data['Type'] == 'METRIC']\n",
    "        \n",
    "        summary_stats.append({\n",
    "            \"Model\": model.split('/')[-1],\n",
    "            \"Provider\": model_data['provider'].iloc[0],\n",
    "            \"Average QPS\": post_data['Requests/s'],\n",
    "            \"Average Response Time\": post_data['Average Response Time'],\n",
    "            \"99th Percentile Latency\": post_data['99%'],\n",
    "            \"Average Tokens per Request\": token_data[token_data['Name'] == 'num_tokens']['Average Response Time'].iloc[0]\n",
    "        })\n",
    "    \n",
    "    # Print comparative summary\n",
    "    print(\"\\nComparative Summary:\")\n",
    "    print(\"-\" * 80)\n",
    "    summary_df = pd.DataFrame(summary_stats)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # Save summary to CSV\n",
    "    summary_df.to_csv(f'{results_dir}/comparative_summary.csv', index=False)\n",
    "    \n",
    "    return summary_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Model and Provider Performance Analysis\n",
    "\n",
    "Evaluate performance metrics of singular model from one provider. This is the most basic benchmark that can be run from the load_test.py script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Make sure to create a .env file in the root directory and add your API keys.\n",
    "For this example, we will use the Fireworks API key.\n",
    "\n",
    "Add the following to your .env file:\n",
    "\n",
    "FIREWORKS_API_KEY=<your_fireworks_api_key>.\n",
    "\n",
    "Alternatively you can edit the following script flags for custom configurations.\n",
    "'''\n",
    "\n",
    "\n",
    "provider_name = \"fireworks\"\n",
    "model_name = \"accounts/fireworks/models/llama-v3p2-3b-instruct\"\n",
    "h = \"https://api.fireworks.ai/inference\" #host url\n",
    "api_key = os.getenv(\"FIREWORKS_API_KEY\")\n",
    "\n",
    "t = \"5s\" #test duration, set to 1 minute for now\n",
    "\n",
    "'''\n",
    "Choose ONE of the following two modes by commenting/uncommenting:\n",
    "'''\n",
    "# MODE 1: Fixed Queries Per Second (QPS)\n",
    "# Use this mode to maintain a steady rate of requests\n",
    "qps = 5  # Target requests per second\n",
    "u = 100   # Number of users (keep high enough to achieve target QPS)\n",
    "s = 100   # Spawn rate (keep high enough to achieve target QPS)\n",
    "\n",
    "# MODE 2: Fixed Concurrency\n",
    "# Use this mode to maintain a steady number of concurrent requests\n",
    "# Comment out Mode 1 above and uncomment below to use this mode\n",
    "'''\n",
    "# QPS does not need to be set for fixed concurrency mode\n",
    "u = 5      # Number of concurrent workers\n",
    "r = 5      # Rate of spawning new workers (workers/second). Look through README.md for more details on spawn rate\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# Create results directory of name single_model_provider_analysis_{TIMESTAMP}\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "edited_model_name = model_name.replace(\"/\", \"_\") if provider_name != \"fireworks\" else model_name.replace(\"accounts/fireworks/models/\", \"\").replace(\"/\", \"_\")\n",
    "\n",
    "\n",
    "\n",
    "results_dir = f\"results/{provider_name}_{edited_model_name}_analysis_{timestamp}\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Construct the command\n",
    "cmd = [\n",
    "    \"locust\",\n",
    "    \"--headless\",       # Run without web UI\n",
    "    \"--only-summary\",   # Only show summary stats\n",
    "    \"-H\", h,           # Host URL\n",
    "    \"--provider\", provider_name,\n",
    "    \"--model\", model_name,\n",
    "    \"--api-key\", api_key,\n",
    "    \"-t\", t,           # Test duration\n",
    "    \"--html\", f\"{results_dir}/report.html\",  # Generate HTML report\n",
    "    \"--csv\", f\"{results_dir}/stats\",        # Generate CSV stats\n",
    "]\n",
    "\n",
    "# Add Mode 1 (Fixed QPS) parameters if uncommented, remember to remove --qps below if using fixed concurrency mode\n",
    "cmd.extend([\n",
    "    \"-u\", str(u),      # Number of users\n",
    "    \"-r\", str(s),      # Spawn rate\n",
    "    \"--qps\", str(qps)  # Target QPS\n",
    "])\n",
    "\n",
    "# Add load_test.py as the locust file\n",
    "locust_file = os.path.join(os.path.dirname(os.getcwd()), \"llm_bench\", \"load_test.py\")\n",
    "cmd.extend([\"-f\", locust_file]) \n",
    "\n",
    "#call our helper function to execute the command\n",
    "success = execute_subprocess(cmd)\n",
    "\n",
    "#Visualize the results\n",
    "if success: \n",
    "    time.sleep(1)\n",
    "    stat_result_paths = [{\"path\": f'{results_dir}/stats_stats.csv', \"config\": {\"provider\": provider_name, \"model\": model_name}}]\n",
    "    visualize_comparative_results(stat_result_paths, results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing different Models and Providers\n",
    "\n",
    "Evaluate performance metrics of different models and providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Edit the provider_configs list to add more providers and models.\n",
    "\n",
    "Add any needed api keys to the .env file. This example uses the Fireworks API key again.\n",
    "'''\n",
    "\n",
    "provider_configs = [\n",
    "    {\"provider\": \"fireworks\", \"model\": \"accounts/fireworks/models/llama-v3p2-3b-instruct\", \"host\": \"https://api.fireworks.ai/inference\", \"api_key\": os.getenv(\"FIREWORKS_API_KEY\")},\n",
    "    {\"provider\": \"fireworks\", \"model\": \"accounts/fireworks/models/mistral-small-24b-instruct-2501\", \"host\": \"https://api.fireworks.ai/inference\", \"api_key\": os.getenv(\"FIREWORKS_API_KEY\")},\n",
    "    #... add more providers and models here\n",
    "]\n",
    "\n",
    "# some starter configs and flags\n",
    "t = \"5s\" #test duration, set to 1 minute for now\n",
    "qps = 5  # Target requests per second\n",
    "u = 100   # Number of users (keep high enough to achieve target QPS)\n",
    "s = 100   # Spawn rate (keep high enough to achieve target QPS)\n",
    "\n",
    "# Create results directory of name single_model_provider_analysis_{TIMESTAMP}\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "results_dir = f\"results/different_models_and_providers_analysis_{timestamp}\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "for index, config in enumerate(provider_configs):\n",
    "    # Construct the command\n",
    "\n",
    "    edited_model_name = config[\"model\"].replace(\"/\", \"_\") if config[\"provider\"] != \"fireworks\" else config[\"model\"].replace(\"accounts/fireworks/models/\", \"\").replace(\"/\", \"_\")\n",
    "    \n",
    "    provider_model_path = f\"{results_dir}/{config[\"provider\"]}_{edited_model_name}_{index}\"\n",
    "    \n",
    "    os.makedirs(provider_model_path, exist_ok=True)\n",
    "    cmd = [\n",
    "        \"locust\",\n",
    "        \"--headless\",       # Run without web UI\n",
    "        \"--only-summary\",   # Only show summary stats\n",
    "        \"-H\", config[\"host\"],           # Host URL\n",
    "        \"--provider\", config[\"provider\"],\n",
    "        \"--model\", config[\"model\"],\n",
    "        \"--api-key\", config[\"api_key\"],\n",
    "        \"-t\", t,           # Test duration\n",
    "        \"--html\", f\"{provider_model_path}/report.html\",  # Generate HTML report\n",
    "        \"--csv\", f\"{provider_model_path}/stats\",        # Generate CSV stats\n",
    "    ]\n",
    "\n",
    "    # Add Mode 1 (Fixed QPS) parameters if uncommented, remember to remove --qps below if using fixed concurrency mode\n",
    "    cmd.extend([\n",
    "        \"-u\", str(u),      # Number of users\n",
    "        \"-r\", str(s),      # Spawn rate\n",
    "        \"--qps\", str(qps)  # Target QPS\n",
    "    ])\n",
    "\n",
    "    # Add load_test.py as the locust file\n",
    "    locust_file = os.path.join(os.path.dirname(os.getcwd()), \"llm_bench\", \"load_test.py\")\n",
    "    cmd.extend([\"-f\", locust_file]) \n",
    "\n",
    "    #call our helper function to execute the command\n",
    "    execute_subprocess(cmd)\n",
    "\n",
    "#Visualize the results\n",
    "stat_result_paths = []\n",
    "for index, config in enumerate(provider_configs):\n",
    "    edited_model_name = config[\"model\"].replace(\"/\", \"_\") if config[\"provider\"] != \"fireworks\" else config[\"model\"].replace(\"accounts/fireworks/models/\", \"\").replace(\"/\", \"_\")\n",
    "    stat_result_paths.append({\"path\": f\"{results_dir}/{config['provider']}_{edited_model_name}_{index}/stats_stats.csv\", \"config\": config})\n",
    "\n",
    "time.sleep(1)\n",
    "visualize_comparative_results(stat_result_paths, results_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Length Analysis\n",
    "\n",
    "Evaluates model performance across different output lengths by testing the same model \n",
    "with varying input token limits (from short to long responses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_token_lengths = [30,64,128,256] # --max-tokens flag\n",
    "max_token_lengths_distribution = \"uniform\" # --max-tokens-distribution flag check readme.md for more details and other options\n",
    "\n",
    "\n",
    "provider_name = \"fireworks\"\n",
    "model_name = \"accounts/fireworks/models/llama-v3p2-3b-instruct\"\n",
    "h = \"https://api.fireworks.ai/inference\" #host url\n",
    "api_key = os.getenv(\"FIREWORKS_API_KEY\")\n",
    "\n",
    "t = \"5s\" #test duration, set to 1 minute for now\n",
    "qps = 5  # Target requests per second\n",
    "u = 100   # Number of users (keep high enough to achieve target QPS)\n",
    "s = 100   # Spawn rate (keep high enough to achieve target QPS)\n",
    "\n",
    "# Create results directory of name single_model_provider_analysis_{TIMESTAMP}\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "results_dir = f\"results/token_length_analysis_{timestamp}\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "for index, token_length in enumerate(max_token_lengths):\n",
    "    # Construct the command\n",
    "\n",
    "    edited_model_name = model_name.replace(\"/\", \"_\") if provider_name != \"fireworks\" else model_name.replace(\"accounts/fireworks/models/\", \"\").replace(\"/\", \"_\")\n",
    "    \n",
    "    token_length_path = f\"{results_dir}/{provider_name}_{edited_model_name}_{token_length}\"\n",
    "    os.makedirs(f\"{token_length_path}\", exist_ok=True)\n",
    "    cmd = [\n",
    "        \"locust\",\n",
    "        \"--headless\",       # Run without web UI\n",
    "        \"--only-summary\",   # Only show summary stats\n",
    "        \"-H\", h,           # Host URL\n",
    "        \"--provider\", provider_name,\n",
    "        \"--model\", model_name,\n",
    "        \"--api-key\", api_key,\n",
    "        \"-t\", t,           # Test duration\n",
    "        \"--max-tokens\", str(token_length), \n",
    "        \"--max-tokens-distribution\", max_token_lengths_distribution,\n",
    "        \"--html\", f\"{token_length_path}/report.html\",  # Generate HTML report\n",
    "        \"--csv\", f\"{token_length_path}/stats\",        # Generate CSV stats\n",
    "    ]\n",
    "\n",
    "    # Add Mode 1 (Fixed QPS) parameters if uncommented, remember to remove --qps below if using fixed concurrency mode\n",
    "    cmd.extend([\n",
    "        \"-u\", str(u),      # Number of users\n",
    "        \"-r\", str(s),      # Spawn rate\n",
    "        \"--qps\", str(qps)  # Target QPS\n",
    "    ])\n",
    "\n",
    "    # Add load_test.py as the locust file\n",
    "    locust_file = os.path.join(os.path.dirname(os.getcwd()), \"llm_bench\", \"load_test.py\")\n",
    "    cmd.extend([\"-f\", locust_file]) \n",
    "\n",
    "    #call our helper function to execute the command\n",
    "    execute_subprocess(cmd)\n",
    "\n",
    "#Visualize the results\n",
    "stat_result_paths = []\n",
    "for index, token_length in enumerate(max_token_lengths):\n",
    "\n",
    "    edited_model_name = model_name.replace(\"/\", \"_\") if provider_name != \"fireworks\" else model_name.replace(\"accounts/fireworks/models/\", \"\").replace(\"/\", \"_\")\n",
    "    stat_result_paths.append({\"path\": f\"{results_dir}/{provider_name}_{edited_model_name}_{token_length}/stats_stats.csv\", \"config\": {\"provider\": \"fireworks\", \"model\": \"accounts/fireworks/models/llama-v3p2-3b-instruct\" + \"_\" + str(token_length)}})\n",
    "\n",
    "time.sleep(1)\n",
    "visualize_comparative_results(stat_result_paths, results_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
